{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the next char\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_mini_batch(train_data, size_of_mini_batch, length_of_sequences):\n",
    "    inputs  = ''\n",
    "    outputs = ''\n",
    "    for _ in range(size_of_mini_batch):\n",
    "        index   = random.randint(0, len(train_data) - length_of_sequences - 1)\n",
    "        inputs  += train_data[index:index + length_of_sequences]\n",
    "        outputs += train_data[index + length_of_sequences]\n",
    "#    print('inputs.len: {}'.format(len(inputs)))\n",
    "    inputs = np.eye(num_of_char)[[ord(x) for x in inputs if ord(x) < 128]]\n",
    "    outputs = np.eye(num_of_char)[[ord(x) for x in outputs if ord(x) < 128]]\n",
    "#    print('inputs.shape: {}'.format(inputs.shape))\n",
    "    inputs  = inputs.reshape(-1, length_of_sequences, 128)\n",
    "    outputs = outputs.reshape(-1, 128)\n",
    "    return (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_prediction_initial(train_data, index, length_of_sequences):\n",
    "    inputs = train_data[index:index + length_of_sequences]\n",
    "    return np.eye(num_of_char)[[ord(x) for x in inputs if ord(x) < 128]]\n",
    "    #return train_data[index:index + length_of_sequences], train_data[index + length_of_sequences + 1]\n",
    "    \n",
    "#inputs, _  = make_prediction_initial(train_data, 0, length_of_initial_sequences)\n",
    "#inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path             = \"./blog.txt\"\n",
    "num_of_char                 = 128\n",
    "num_of_input_nodes          = 128\n",
    "num_of_hidden_nodes         = 512\n",
    "num_of_output_nodes         = 128\n",
    "length_of_sequences         = 50\n",
    "num_of_training_epochs      = 2000\n",
    "length_of_initial_sequences = 50\n",
    "num_of_prediction_epochs    = 1000\n",
    "size_of_mini_batch          = 20\n",
    "learning_rate               = 0.002\n",
    "forget_bias                 = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_data = np.load(train_data_path)\n",
    "f = open(train_data_path)\n",
    "text = f.read()\n",
    "f.close()\n",
    "train_data = ''.join([x for x in text if ord(x) < 128])\n",
    "#train_data = np.eye(num_of_char)[[ord(x) for x in text if ord(x) < 128]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_ph, size_of_mini_batch_ph, lstm_output_keep_prob):\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "        weight1_var = tf.Variable(tf.truncated_normal([num_of_input_nodes, num_of_hidden_nodes], stddev=0.1), name=\"weight1\")\n",
    "        weight2_var = tf.Variable(tf.truncated_normal([num_of_hidden_nodes, num_of_output_nodes], stddev=0.1), name=\"weight2\")\n",
    "        bias1_var   = tf.Variable(tf.truncated_normal([num_of_hidden_nodes], stddev=0.1), name=\"bias1\")\n",
    "        bias2_var   = tf.Variable(tf.truncated_normal([num_of_output_nodes], stddev=0.1), name=\"bias2\")\n",
    "\n",
    "        weight1_hist = tf.histogram_summary(\"layer1/weights\", weight1_var)\n",
    "        weight2_hist = tf.histogram_summary(\"layer2/weights\", weight2_var)\n",
    "        bias1_hist = tf.histogram_summary(\"layer1/biases\", bias1_var)\n",
    "        bias2_hist = tf.histogram_summary(\"layer2/biases\", bias2_var)\n",
    "        \n",
    "        # pre rnn\n",
    "        in1 = tf.transpose(input_ph, [1, 0, 2])         # (batch, sequence, data) -> (sequence, batch, data)\n",
    "        in2 = tf.reshape(in1, [-1, num_of_input_nodes]) # (sequence, batch, data) -> (sequence * batch, data)\n",
    "        in3 = tf.matmul(in2, weight1_var) + bias1_var\n",
    "        in4 = tf.split(0, length_of_sequences, in3)     # sequence * (batch, data)\n",
    "\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_of_hidden_nodes, forget_bias=forget_bias, state_is_tuple=True)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=lstm_output_keep_prob)\n",
    "        #cell = tf.nn.rnn_cell.MultiRNNCell([cell] * 2, state_is_tuple=True)\n",
    "        initial_state = cell.zero_state(size_of_mini_batch_ph, tf.float32)\n",
    "        rnn_output, states_op = tf.nn.rnn(cell, in4, initial_state=initial_state, dtype=tf.float32)\n",
    "        \n",
    "        # post rnn\n",
    "        #output_op = tf.matmul(rnn_output[-1], weight2_var) + bias2_var\n",
    "        out1 = tf.matmul(rnn_output[-1], weight2_var) + bias2_var\n",
    "        output_op = out1\n",
    "        #output_argmax = tf.argmax(out1, 0)\n",
    "\n",
    "    return output_op, rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output_op):\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_op, supervisor_ph))\n",
    "        loss_op = cross_entropy\n",
    "        tf.scalar_summary(\"loss\", loss_op)\n",
    "    return loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(loss_op):\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "    return training_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model with fresh parameters.\n",
      "train#0, train loss: 3.504850e+00\n",
      "train#10, train loss: 3.031415e+00\n",
      "train#20, train loss: 3.225258e+00\n",
      "train#30, train loss: 2.995014e+00\n",
      "train#40, train loss: 2.963679e+00\n",
      "train#50, train loss: 2.794437e+00\n",
      "train#60, train loss: 3.004005e+00\n",
      "train#70, train loss: 2.648190e+00\n",
      "train#80, train loss: 3.173921e+00\n",
      "train#90, train loss: 2.763678e+00\n",
      "train#100, train loss: 2.304136e+00\n",
      "train#110, train loss: 2.966321e+00\n",
      "train#120, train loss: 2.183220e+00\n",
      "train#130, train loss: 2.707037e+00\n",
      "train#140, train loss: 2.485229e+00\n",
      "train#150, train loss: 2.384695e+00\n",
      "train#160, train loss: 1.724151e+00\n",
      "train#170, train loss: 2.464612e+00\n",
      "train#180, train loss: 2.341499e+00\n",
      "train#190, train loss: 2.465981e+00\n",
      "train#200, train loss: 2.759062e+00\n",
      "train#210, train loss: 2.670258e+00\n",
      "train#220, train loss: 2.558647e+00\n",
      "train#230, train loss: 2.093037e+00\n",
      "train#240, train loss: 2.279039e+00\n",
      "train#250, train loss: 2.744504e+00\n",
      "train#260, train loss: 2.666466e+00\n",
      "train#270, train loss: 2.305228e+00\n",
      "train#280, train loss: 2.368317e+00\n",
      "train#290, train loss: 2.215332e+00\n",
      "train#300, train loss: 3.209289e+00\n",
      "train#310, train loss: 2.744258e+00\n",
      "train#320, train loss: 2.166743e+00\n",
      "train#330, train loss: 2.212858e+00\n",
      "train#340, train loss: 2.298334e+00\n",
      "train#350, train loss: 1.968822e+00\n",
      "train#360, train loss: 2.314392e+00\n",
      "train#370, train loss: 1.575188e+00\n",
      "train#380, train loss: 1.960037e+00\n",
      "train#390, train loss: 1.616954e+00\n",
      "train#400, train loss: 1.793654e+00\n",
      "train#410, train loss: 2.083618e+00\n",
      "train#420, train loss: 1.498095e+00\n",
      "train#430, train loss: 1.468533e+00\n",
      "train#440, train loss: 1.555313e+00\n",
      "train#450, train loss: 1.505562e+00\n",
      "train#460, train loss: 1.774202e+00\n",
      "train#470, train loss: 1.812358e+00\n",
      "train#480, train loss: 9.029326e-01\n",
      "train#490, train loss: 2.090527e+00\n",
      "train#500, train loss: 2.386674e+00\n",
      "train#510, train loss: 1.656192e+00\n",
      "train#520, train loss: 1.816232e+00\n",
      "train#530, train loss: 1.699209e+00\n",
      "train#540, train loss: 2.665355e+00\n",
      "train#550, train loss: 1.632409e+00\n",
      "train#560, train loss: 1.525181e+00\n",
      "train#570, train loss: 1.666869e+00\n",
      "train#580, train loss: 9.855448e-01\n",
      "train#590, train loss: 1.661527e+00\n",
      "train#600, train loss: 1.481458e+00\n",
      "train#610, train loss: 1.987009e+00\n",
      "train#620, train loss: 1.486150e+00\n",
      "train#630, train loss: 1.617249e+00\n",
      "train#640, train loss: 1.501256e+00\n",
      "train#650, train loss: 9.726095e-01\n",
      "train#660, train loss: 1.234642e+00\n",
      "train#670, train loss: 1.101969e+00\n",
      "train#680, train loss: 1.404250e+00\n",
      "train#690, train loss: 1.370002e+00\n",
      "train#700, train loss: 1.008057e+00\n",
      "train#710, train loss: 1.690153e+00\n",
      "train#720, train loss: 8.645999e-01\n",
      "train#730, train loss: 2.379212e+00\n",
      "train#740, train loss: 1.982934e+00\n",
      "train#750, train loss: 9.173682e-01\n",
      "train#760, train loss: 1.183828e+00\n",
      "train#770, train loss: 7.901707e-01\n",
      "train#780, train loss: 1.043160e+00\n",
      "train#790, train loss: 8.770776e-01\n",
      "train#800, train loss: 9.702123e-01\n",
      "train#810, train loss: 1.499860e+00\n",
      "train#820, train loss: 1.037274e+00\n",
      "train#830, train loss: 1.967571e+00\n",
      "train#840, train loss: 1.534228e+00\n",
      "train#850, train loss: 1.464060e+00\n",
      "train#860, train loss: 1.024984e+00\n",
      "train#870, train loss: 1.428968e+00\n",
      "train#880, train loss: 1.130466e+00\n",
      "train#890, train loss: 1.382385e+00\n",
      "train#900, train loss: 1.578384e+00\n",
      "train#910, train loss: 1.425279e+00\n",
      "train#920, train loss: 1.829937e+00\n",
      "train#930, train loss: 1.591941e+00\n",
      "train#940, train loss: 1.140553e+00\n",
      "train#950, train loss: 1.318189e+00\n",
      "train#960, train loss: 1.318310e+00\n",
      "train#970, train loss: 8.712352e-01\n",
      "train#980, train loss: 1.181469e+00\n",
      "train#990, train loss: 7.533056e-01\n",
      "train#1000, train loss: 1.423444e+00\n",
      "train#1010, train loss: 1.206601e+00\n",
      "train#1020, train loss: 1.068678e+00\n",
      "train#1030, train loss: 1.222409e+00\n",
      "train#1040, train loss: 9.062236e-01\n",
      "train#1050, train loss: 4.849681e-01\n",
      "train#1060, train loss: 9.626930e-01\n",
      "train#1070, train loss: 1.063582e+00\n",
      "train#1080, train loss: 1.131219e+00\n",
      "train#1090, train loss: 1.140368e+00\n",
      "train#1100, train loss: 1.426873e+00\n",
      "train#1110, train loss: 8.462110e-01\n",
      "train#1120, train loss: 6.248378e-01\n",
      "train#1130, train loss: 8.891293e-01\n",
      "train#1140, train loss: 1.072839e+00\n",
      "train#1150, train loss: 1.090200e+00\n",
      "train#1160, train loss: 7.161244e-01\n",
      "train#1170, train loss: 6.835478e-01\n",
      "train#1180, train loss: 9.708951e-01\n",
      "train#1190, train loss: 1.080042e+00\n",
      "train#1200, train loss: 1.242888e+00\n",
      "train#1210, train loss: 1.313075e+00\n",
      "train#1220, train loss: 1.514708e+00\n",
      "train#1230, train loss: 9.664755e-01\n",
      "train#1240, train loss: 9.676421e-01\n",
      "train#1250, train loss: 1.063208e+00\n",
      "train#1260, train loss: 7.321479e-01\n",
      "train#1270, train loss: 1.009535e+00\n",
      "train#1280, train loss: 8.669565e-01\n",
      "train#1290, train loss: 8.251265e-01\n",
      "train#1300, train loss: 5.772916e-01\n",
      "train#1310, train loss: 1.153972e+00\n",
      "train#1320, train loss: 7.071617e-01\n",
      "train#1330, train loss: 9.473808e-01\n",
      "train#1340, train loss: 1.151641e+00\n",
      "train#1350, train loss: 7.109433e-01\n",
      "train#1360, train loss: 5.426214e-01\n",
      "train#1370, train loss: 7.853443e-01\n",
      "train#1380, train loss: 6.842272e-01\n",
      "train#1390, train loss: 1.554198e+00\n",
      "train#1400, train loss: 8.391182e-01\n",
      "train#1410, train loss: 8.886309e-01\n",
      "train#1420, train loss: 6.638292e-01\n",
      "train#1430, train loss: 4.132455e-01\n",
      "train#1440, train loss: 1.090904e+00\n",
      "train#1450, train loss: 4.421234e-01\n",
      "train#1460, train loss: 7.303556e-01\n",
      "train#1470, train loss: 6.487526e-01\n",
      "train#1480, train loss: 6.777340e-01\n",
      "train#1490, train loss: 8.445826e-01\n",
      "train#1500, train loss: 6.925122e-01\n",
      "train#1510, train loss: 1.065385e+00\n",
      "train#1520, train loss: 4.454755e-01\n",
      "train#1530, train loss: 8.826138e-01\n",
      "train#1540, train loss: 9.255015e-01\n",
      "train#1550, train loss: 8.081449e-01\n",
      "train#1560, train loss: 7.629727e-01\n",
      "train#1570, train loss: 9.078305e-01\n",
      "train#1580, train loss: 7.812871e-01\n",
      "train#1590, train loss: 8.651078e-01\n",
      "train#1600, train loss: 7.596650e-01\n",
      "train#1610, train loss: 5.754753e-01\n",
      "train#1620, train loss: 6.928109e-01\n",
      "train#1630, train loss: 7.025166e-01\n",
      "train#1640, train loss: 5.793883e-01\n",
      "train#1650, train loss: 8.308932e-01\n",
      "train#1660, train loss: 8.363158e-01\n",
      "train#1670, train loss: 4.989932e-01\n",
      "train#1680, train loss: 4.507190e-01\n",
      "train#1690, train loss: 6.217116e-01\n",
      "train#1700, train loss: 6.710678e-01\n",
      "train#1710, train loss: 6.973513e-01\n",
      "train#1720, train loss: 7.798213e-01\n",
      "train#1730, train loss: 6.588566e-01\n",
      "train#1740, train loss: 9.607544e-01\n",
      "train#1750, train loss: 7.573661e-01\n",
      "train#1760, train loss: 6.735462e-01\n",
      "train#1770, train loss: 8.004036e-01\n",
      "train#1780, train loss: 4.881880e-01\n",
      "train#1790, train loss: 6.393509e-01\n",
      "train#1800, train loss: 4.086866e-01\n",
      "train#1810, train loss: 1.078076e+00\n",
      "train#1820, train loss: 3.933540e-01\n",
      "train#1830, train loss: 5.469280e-01\n",
      "train#1840, train loss: 5.461634e-01\n",
      "train#1850, train loss: 4.567686e-01\n",
      "train#1860, train loss: 6.087117e-01\n",
      "train#1870, train loss: 4.214132e-01\n",
      "train#1880, train loss: 4.604771e-01\n",
      "train#1890, train loss: 5.441776e-01\n",
      "train#1900, train loss: 3.591912e-01\n",
      "train#1910, train loss: 3.346725e-01\n",
      "train#1920, train loss: 7.733525e-01\n",
      "train#1930, train loss: 4.734040e-01\n",
      "train#1940, train loss: 4.916069e-01\n",
      "train#1950, train loss: 6.285906e-01\n",
      "train#1960, train loss: 5.081116e-01\n",
      "train#1970, train loss: 8.448914e-01\n",
      "train#1980, train loss: 5.883950e-01\n",
      "train#1990, train loss: 2.545722e-01\n",
      "prediction: \n",
      "('outputs:', 'are encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features into device encryption Direct Boot pass this security features ')\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    input_ph      = tf.placeholder(tf.float32, [None, length_of_sequences, num_of_input_nodes], name=\"input\")\n",
    "    supervisor_ph = tf.placeholder(tf.float32, [None, num_of_output_nodes], name=\"supervisor\")\n",
    "    size_of_mini_batch_ph = tf.placeholder(tf.int32, name=\"size_of_mini_batch\")\n",
    "    lstm_output_keep_prob_ph = tf.placeholder(tf.float32, name=\"lstm_output_keep_prob\")\n",
    "\n",
    "    output_op, rnn_output = inference(input_ph, size_of_mini_batch_ph, lstm_output_keep_prob_ph)\n",
    "    loss_op = loss(output_op)\n",
    "    training_op = train(loss_op)\n",
    "\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    #istate_ph = cell.zero_state(size_of_mini_batch, tf.float32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.train.SummaryWriter(\"data\", graph=sess.graph)\n",
    "        \"\"\"\n",
    "        # random seed fix\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        \"\"\"\n",
    "        epoch_start = 0\n",
    "        ckpt = tf.train.get_checkpoint_state(\"./train/\")\n",
    "        saver = tf.train.Saver()\n",
    "        if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "            print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            epoch_start = int(re.search(r\"(\\d+)$\", ckpt.model_checkpoint_path).group())\n",
    "        else:\n",
    "            print(\"Created model with fresh parameters.\")\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "        # init variables\n",
    "\n",
    "        for epoch in range(epoch_start, num_of_training_epochs):\n",
    "            inputs, supervisors = make_mini_batch(train_data, size_of_mini_batch, length_of_sequences)\n",
    "            #state = sess.run(istate_ph)\n",
    "\n",
    "            train_dict = {\n",
    "                input_ph:      inputs,\n",
    "                supervisor_ph: supervisors,\n",
    "                size_of_mini_batch_ph: size_of_mini_batch,\n",
    "                lstm_output_keep_prob_ph: 0.5,\n",
    "            }\n",
    "            rnnout, _ = sess.run([rnn_output, training_op], feed_dict=train_dict)\n",
    "#            print(\"rnnout: {}\".format(rnnout[-1]))\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                summary_str, train_loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n",
    "                summary_writer.add_summary(summary_str, epoch)\n",
    "                print(\"train#%d, train loss: %e\" % (epoch, train_loss))\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                saver.save(sess, \"train/translate.ckpt\", global_step=epoch)\n",
    "\n",
    "        inputs = make_prediction_initial(train_data, 0, length_of_initial_sequences)\n",
    "#        outputs = np.empty(0)\n",
    "        outputs = np.array([[0 for _ in range(128)]])\n",
    "\n",
    "#        istate_ph = cell.zero_state(1, tf.float32)\n",
    "#        sess.run(istate_ph)\n",
    "\n",
    "        print(\"prediction: \")\n",
    "        for epoch in range(num_of_prediction_epochs):\n",
    "            pred_dict = {\n",
    "                input_ph:  [inputs],\n",
    "                size_of_mini_batch_ph: 1,\n",
    "                lstm_output_keep_prob_ph: 1.0\n",
    "            }\n",
    "#            output, states = sess.run([output_op, states_op], feed_dict=pred_dict)\n",
    "            output = sess.run(output_op, feed_dict=pred_dict)\n",
    "#            print(output)\n",
    "            output_onehotvec = np.eye(num_of_char)[[np.argmax(output)]]\n",
    "            inputs  = np.delete(inputs, 0, 0)\n",
    "            #inputs  = np.append(inputs, output_onehotvec, 0)\n",
    "            inputs  = np.append(inputs, output_onehotvec, 0)\n",
    "            outputs = np.append(outputs, output_onehotvec, 0)\n",
    "        \n",
    "        outputs  = np.delete(outputs, 0, 0)\n",
    "        output_ascii = np.argmax(outputs.reshape(num_of_prediction_epochs, num_of_output_nodes), axis=1)\n",
    "#        print(\"asciis: \", output_ascii)\n",
    "#        print(\"outputs:\", [chr(x) for x in output_ascii])\n",
    "        print(\"outputs:\", ''.join([chr(x) for x in output_ascii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (50, 128)\n",
      "outputs: (100, 128)\n",
      "output: (1, 128)\n",
      "onehotvec: (1, 128)\n"
     ]
    }
   ],
   "source": [
    "ins = inputs\n",
    "outs = outputs\n",
    "out = output\n",
    "out_ohv = output_onehotvec\n",
    "\n",
    "print(\"inputs: {}\".format(ins.shape))\n",
    "print(\"outputs: {}\".format(outs.shape))\n",
    "print(\"output: {}\".format(out.shape))\n",
    "print(\"onehotvec: {}\".format(out_ohv.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
