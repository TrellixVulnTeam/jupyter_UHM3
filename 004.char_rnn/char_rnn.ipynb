{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the next char\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_mini_batch(train_data, size_of_mini_batch, length_of_sequences):\n",
    "    inputs  = ''\n",
    "    outputs = ''\n",
    "    for _ in range(size_of_mini_batch):\n",
    "        index   = random.randint(0, len(train_data) - length_of_sequences - 1)\n",
    "        inputs  += train_data[index:index + length_of_sequences]\n",
    "        outputs += train_data[index + length_of_sequences]\n",
    "    inputs = np.eye(num_of_char)[[ord(x) for x in inputs if ord(x) < 128]]\n",
    "    outputs = np.eye(num_of_char)[[ord(x) for x in outputs if ord(x) < 128]]\n",
    "    inputs  = inputs.reshape(-1, length_of_sequences, 128)\n",
    "    outputs = outputs.reshape(-1, 128)\n",
    "    return (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_prediction_initial(train_data, index, length_of_sequences):\n",
    "    inputs = train_data[index:index + length_of_sequences]\n",
    "    return np.eye(num_of_char)[[ord(x) for x in inputs if ord(x) < 128]]    \n",
    "#inputs, _  = make_prediction_initial(train_data, 0, length_of_initial_sequences)\n",
    "#inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path             = \"./input/stack.tex\"\n",
    "num_of_char                 = 128\n",
    "num_of_input_nodes          = 128\n",
    "num_of_hidden_nodes         = 512\n",
    "num_of_output_nodes         = 128\n",
    "length_of_sequences         = 50\n",
    "num_of_training_epochs      = 14000\n",
    "length_of_initial_sequences = 50\n",
    "num_of_prediction_epochs    = 1000\n",
    "size_of_mini_batch          = 20\n",
    "learning_rate               = 0.0005\n",
    "forget_bias                 = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_data = np.load(train_data_path)\n",
    "f = open(train_data_path)\n",
    "text = f.read()\n",
    "f.close()\n",
    "train_data = ''.join([x for x in text if ord(x) < 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_ph, size_of_mini_batch_ph, lstm_output_keep_prob):\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "        weight1_var = tf.Variable(tf.truncated_normal([num_of_input_nodes, num_of_hidden_nodes], stddev=0.1), name=\"weight1\")\n",
    "        weight2_var = tf.Variable(tf.truncated_normal([num_of_hidden_nodes, num_of_output_nodes], stddev=0.1), name=\"weight2\")\n",
    "        bias1_var   = tf.Variable(tf.truncated_normal([num_of_hidden_nodes], stddev=0.1), name=\"bias1\")\n",
    "        bias2_var   = tf.Variable(tf.truncated_normal([num_of_output_nodes], stddev=0.1), name=\"bias2\")\n",
    "\n",
    "        weight1_hist = tf.histogram_summary(\"layer1/weights\", weight1_var)\n",
    "        weight2_hist = tf.histogram_summary(\"layer2/weights\", weight2_var)\n",
    "        bias1_hist = tf.histogram_summary(\"layer1/biases\", bias1_var)\n",
    "        bias2_hist = tf.histogram_summary(\"layer2/biases\", bias2_var)\n",
    "        \n",
    "        # pre rnn\n",
    "        in1 = tf.transpose(input_ph, [1, 0, 2])         # (batch, sequence, data) -> (sequence, batch, data)\n",
    "        in2 = tf.reshape(in1, [-1, num_of_input_nodes]) # (sequence, batch, data) -> (sequence * batch, data)\n",
    "        in3 = tf.matmul(in2, weight1_var) + bias1_var\n",
    "        in4 = tf.split(0, length_of_sequences, in3)     # sequence * (batch, data)\n",
    "\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_of_hidden_nodes, forget_bias=forget_bias, state_is_tuple=True)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=lstm_output_keep_prob)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * 2, state_is_tuple=True)\n",
    "        initial_state = cell.zero_state(size_of_mini_batch_ph, tf.float32)\n",
    "        rnn_output, states_op = tf.nn.rnn(cell, in4, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "        output_op = tf.matmul(rnn_output[-1], weight2_var) + bias2_var\n",
    "    return output_op, rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output_op):\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_op, supervisor_ph))\n",
    "        loss_op = cross_entropy\n",
    "        tf.scalar_summary(\"loss\", loss_op)\n",
    "    return loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(loss_op):\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "    return training_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from ./train/translate.ckpt-10990\n",
      "train#10990, train loss: 1.163687e+00\n",
      "train#11000, train loss: 9.561481e-01\n",
      "train#11010, train loss: 8.963711e-01\n",
      "train#11020, train loss: 1.968876e+00\n",
      "train#11030, train loss: 8.193018e-01\n",
      "train#11040, train loss: 9.574386e-01\n",
      "train#11050, train loss: 1.119974e+00\n",
      "train#11060, train loss: 1.226853e+00\n",
      "train#11070, train loss: 7.975789e-01\n",
      "train#11080, train loss: 1.182474e+00\n",
      "train#11090, train loss: 6.701686e-01\n",
      "train#11100, train loss: 6.690273e-01\n",
      "train#11110, train loss: 8.425705e-01\n",
      "train#11120, train loss: 4.172142e-01\n",
      "train#11130, train loss: 1.294003e+00\n",
      "train#11140, train loss: 6.723601e-01\n",
      "train#11150, train loss: 6.293373e-01\n",
      "train#11160, train loss: 1.551203e+00\n",
      "train#11170, train loss: 1.132574e+00\n",
      "train#11180, train loss: 8.830321e-01\n",
      "train#11190, train loss: 3.862577e-01\n",
      "train#11200, train loss: 7.655944e-01\n",
      "train#11210, train loss: 1.494378e+00\n",
      "train#11220, train loss: 4.885359e-01\n",
      "train#11230, train loss: 1.207722e+00\n",
      "train#11240, train loss: 1.010359e+00\n",
      "train#11250, train loss: 7.271088e-01\n",
      "train#11260, train loss: 1.188770e+00\n",
      "train#11270, train loss: 8.729658e-01\n",
      "train#11280, train loss: 1.090502e+00\n",
      "train#11290, train loss: 1.688818e+00\n",
      "train#11300, train loss: 3.207878e-01\n",
      "train#11310, train loss: 8.683552e-01\n",
      "train#11320, train loss: 1.201709e+00\n",
      "train#11330, train loss: 1.115073e+00\n",
      "train#11340, train loss: 3.896853e-01\n",
      "train#11350, train loss: 6.246921e-01\n",
      "train#11360, train loss: 6.186067e-01\n",
      "train#11370, train loss: 7.454930e-01\n",
      "train#11380, train loss: 5.678259e-01\n",
      "train#11390, train loss: 8.094629e-01\n",
      "train#11400, train loss: 1.190665e+00\n",
      "train#11410, train loss: 9.856662e-01\n",
      "train#11420, train loss: 9.936405e-01\n",
      "train#11430, train loss: 6.682934e-01\n",
      "train#11440, train loss: 1.293048e+00\n",
      "train#11450, train loss: 5.511526e-01\n",
      "train#11460, train loss: 5.951391e-01\n",
      "train#11470, train loss: 1.243889e+00\n",
      "train#11480, train loss: 7.936822e-01\n",
      "train#11490, train loss: 1.024612e+00\n",
      "train#11500, train loss: 9.869789e-01\n",
      "train#11510, train loss: 4.913248e-01\n",
      "train#11520, train loss: 7.400772e-01\n",
      "train#11530, train loss: 9.698094e-01\n",
      "train#11540, train loss: 1.004390e+00\n",
      "train#11550, train loss: 5.123159e-01\n",
      "train#11560, train loss: 3.162836e-01\n",
      "train#11570, train loss: 4.857818e-01\n",
      "train#11580, train loss: 6.585338e-01\n",
      "train#11590, train loss: 7.461228e-01\n",
      "train#11600, train loss: 2.936272e-01\n",
      "train#11610, train loss: 7.487923e-01\n",
      "train#11620, train loss: 4.575220e-01\n",
      "train#11630, train loss: 6.596280e-01\n",
      "train#11640, train loss: 5.189430e-01\n",
      "train#11650, train loss: 8.219083e-01\n",
      "train#11660, train loss: 6.815686e-01\n",
      "train#11670, train loss: 1.286083e+00\n",
      "train#11680, train loss: 1.150122e+00\n",
      "train#11690, train loss: 1.052297e+00\n",
      "train#11700, train loss: 1.185058e+00\n",
      "train#11710, train loss: 8.557913e-01\n",
      "train#11720, train loss: 8.101119e-01\n",
      "train#11730, train loss: 1.001477e+00\n",
      "train#11740, train loss: 1.048787e+00\n",
      "train#11750, train loss: 8.680918e-01\n",
      "train#11760, train loss: 1.474072e+00\n",
      "train#11770, train loss: 6.339473e-01\n",
      "train#11780, train loss: 7.928783e-01\n",
      "train#11790, train loss: 9.285811e-01\n",
      "train#11800, train loss: 8.491670e-01\n",
      "train#11810, train loss: 7.469797e-01\n",
      "train#11820, train loss: 5.992299e-01\n",
      "train#11830, train loss: 1.057750e+00\n",
      "train#11840, train loss: 7.261797e-01\n",
      "train#11850, train loss: 6.187016e-01\n",
      "train#11860, train loss: 1.172637e+00\n",
      "train#11870, train loss: 8.806568e-01\n",
      "train#11880, train loss: 9.965681e-01\n",
      "train#11890, train loss: 9.849776e-01\n",
      "train#11900, train loss: 7.775289e-01\n",
      "train#11910, train loss: 1.115331e+00\n",
      "train#11920, train loss: 1.812084e+00\n",
      "train#11930, train loss: 5.546382e-01\n",
      "train#11940, train loss: 1.170011e+00\n",
      "train#11950, train loss: 6.439524e-01\n",
      "train#11960, train loss: 1.058411e+00\n",
      "train#11970, train loss: 6.043761e-01\n",
      "train#11980, train loss: 6.101801e-01\n",
      "train#11990, train loss: 5.667291e-01\n",
      "train#12000, train loss: 9.075688e-01\n",
      "train#12010, train loss: 7.171172e-01\n",
      "train#12020, train loss: 3.890821e-01\n",
      "train#12030, train loss: 8.965992e-01\n",
      "train#12040, train loss: 5.846702e-01\n",
      "train#12050, train loss: 1.264729e+00\n",
      "train#12060, train loss: 3.768690e-01\n",
      "train#12070, train loss: 8.043076e-01\n",
      "train#12080, train loss: 9.451846e-01\n",
      "train#12090, train loss: 1.148057e+00\n",
      "train#12100, train loss: 1.090145e+00\n",
      "train#12110, train loss: 5.268335e-01\n",
      "train#12120, train loss: 6.417756e-01\n",
      "train#12130, train loss: 1.192424e+00\n",
      "train#12140, train loss: 5.536529e-01\n",
      "train#12150, train loss: 4.757974e-01\n",
      "train#12160, train loss: 1.454824e+00\n",
      "train#12170, train loss: 9.994532e-01\n",
      "train#12180, train loss: 6.197589e-01\n",
      "train#12190, train loss: 8.946625e-01\n",
      "train#12200, train loss: 8.504568e-01\n",
      "train#12210, train loss: 9.631345e-01\n",
      "train#12220, train loss: 1.408202e+00\n",
      "train#12230, train loss: 1.024099e+00\n",
      "train#12240, train loss: 8.014922e-01\n",
      "train#12250, train loss: 1.091269e+00\n",
      "train#12260, train loss: 7.854278e-01\n",
      "train#12270, train loss: 1.372398e+00\n",
      "train#12280, train loss: 1.139946e+00\n",
      "train#12290, train loss: 8.512829e-01\n",
      "train#12300, train loss: 7.913412e-01\n",
      "train#12310, train loss: 9.970928e-01\n",
      "train#12320, train loss: 8.148489e-01\n",
      "train#12330, train loss: 8.694344e-01\n",
      "train#12340, train loss: 9.290560e-01\n",
      "train#12350, train loss: 1.025390e+00\n",
      "train#12360, train loss: 1.061227e+00\n",
      "train#12370, train loss: 3.385436e-01\n",
      "train#12380, train loss: 5.927314e-01\n",
      "train#12390, train loss: 1.456710e+00\n",
      "train#12400, train loss: 9.636114e-01\n",
      "train#12410, train loss: 1.081760e+00\n",
      "train#12420, train loss: 6.824046e-01\n",
      "train#12430, train loss: 5.810330e-01\n",
      "train#12440, train loss: 1.039601e+00\n",
      "train#12450, train loss: 9.725264e-01\n",
      "train#12460, train loss: 7.986957e-01\n",
      "train#12470, train loss: 9.901868e-01\n",
      "train#12480, train loss: 1.094265e+00\n",
      "train#12490, train loss: 8.011730e-01\n",
      "train#12500, train loss: 8.762352e-01\n",
      "train#12510, train loss: 1.153272e+00\n",
      "train#12520, train loss: 1.409467e+00\n",
      "train#12530, train loss: 3.949160e-01\n",
      "train#12540, train loss: 3.410901e-01\n",
      "train#12550, train loss: 9.789351e-01\n",
      "train#12560, train loss: 2.235545e-01\n",
      "train#12570, train loss: 9.527296e-01\n",
      "train#12580, train loss: 1.446262e+00\n",
      "train#12590, train loss: 7.156155e-01\n",
      "train#12600, train loss: 5.752463e-01\n",
      "train#12610, train loss: 4.283909e-01\n",
      "train#12620, train loss: 8.978993e-01\n",
      "train#12630, train loss: 8.908278e-01\n",
      "train#12640, train loss: 3.030591e-01\n",
      "train#12650, train loss: 5.385677e-01\n",
      "train#12660, train loss: 6.779357e-01\n",
      "train#12670, train loss: 3.939807e-01\n",
      "train#12680, train loss: 4.025161e-01\n",
      "train#12690, train loss: 8.122349e-01\n",
      "train#12700, train loss: 9.502851e-01\n",
      "train#12710, train loss: 1.238281e+00\n",
      "train#12720, train loss: 6.266990e-01\n",
      "train#12730, train loss: 5.157191e-01\n",
      "train#12740, train loss: 7.630596e-01\n",
      "train#12750, train loss: 1.227208e+00\n",
      "train#12760, train loss: 6.893179e-01\n",
      "train#12770, train loss: 8.867636e-01\n",
      "train#12780, train loss: 7.570928e-01\n",
      "train#12790, train loss: 7.309853e-01\n",
      "train#12800, train loss: 1.254907e+00\n",
      "train#12810, train loss: 1.055952e+00\n",
      "train#12820, train loss: 3.950814e-01\n",
      "train#12830, train loss: 1.051550e+00\n",
      "train#12840, train loss: 8.010226e-01\n",
      "train#12850, train loss: 1.100811e+00\n",
      "train#12860, train loss: 6.818534e-01\n",
      "train#12870, train loss: 5.636416e-01\n",
      "train#12880, train loss: 5.394746e-01\n",
      "train#12890, train loss: 9.999129e-01\n",
      "train#12900, train loss: 4.233555e-01\n",
      "train#12910, train loss: 1.011204e+00\n",
      "train#12920, train loss: 6.668913e-01\n",
      "train#12930, train loss: 7.125529e-01\n",
      "train#12940, train loss: 1.012576e+00\n",
      "train#12950, train loss: 4.486218e-01\n",
      "train#12960, train loss: 1.065596e+00\n",
      "train#12970, train loss: 1.019013e+00\n",
      "train#12980, train loss: 6.903332e-01\n",
      "train#12990, train loss: 9.322182e-01\n",
      "train#13000, train loss: 6.588833e-01\n",
      "train#13010, train loss: 8.571137e-01\n",
      "train#13020, train loss: 1.027257e+00\n",
      "train#13030, train loss: 6.690456e-01\n",
      "train#13040, train loss: 1.267317e+00\n",
      "train#13050, train loss: 1.522877e+00\n",
      "train#13060, train loss: 7.462964e-01\n",
      "train#13070, train loss: 4.628810e-01\n",
      "train#13080, train loss: 7.439641e-01\n",
      "train#13090, train loss: 5.802534e-01\n",
      "train#13100, train loss: 1.572736e+00\n",
      "train#13110, train loss: 4.369215e-01\n",
      "train#13120, train loss: 5.680199e-01\n",
      "train#13130, train loss: 8.566008e-01\n",
      "train#13140, train loss: 7.907408e-01\n",
      "train#13150, train loss: 7.556856e-01\n",
      "train#13160, train loss: 8.608076e-01\n",
      "train#13170, train loss: 4.707890e-01\n",
      "train#13180, train loss: 8.731586e-01\n",
      "train#13190, train loss: 1.295603e+00\n",
      "train#13200, train loss: 4.699652e-01\n",
      "train#13210, train loss: 6.845208e-01\n",
      "train#13220, train loss: 3.559923e-01\n",
      "train#13230, train loss: 1.187588e+00\n",
      "train#13240, train loss: 6.647068e-01\n",
      "train#13250, train loss: 7.542495e-01\n",
      "train#13260, train loss: 4.646606e-01\n",
      "train#13270, train loss: 1.389363e+00\n",
      "train#13280, train loss: 6.058702e-01\n",
      "train#13290, train loss: 1.052270e+00\n",
      "train#13300, train loss: 1.009032e+00\n",
      "train#13310, train loss: 6.215069e-01\n",
      "train#13320, train loss: 9.236671e-01\n",
      "train#13330, train loss: 1.286313e+00\n",
      "train#13340, train loss: 9.366047e-01\n",
      "train#13350, train loss: 7.011715e-01\n",
      "train#13360, train loss: 7.576236e-01\n",
      "train#13370, train loss: 1.235258e+00\n",
      "train#13380, train loss: 8.857222e-01\n",
      "train#13390, train loss: 4.885546e-01\n",
      "train#13400, train loss: 5.580283e-01\n",
      "train#13410, train loss: 1.349892e+00\n",
      "train#13420, train loss: 7.940328e-01\n",
      "train#13430, train loss: 9.125822e-01\n",
      "train#13440, train loss: 1.014793e+00\n",
      "train#13450, train loss: 9.146448e-01\n",
      "train#13460, train loss: 7.194404e-01\n",
      "train#13470, train loss: 6.160759e-01\n",
      "train#13480, train loss: 9.539248e-01\n",
      "train#13490, train loss: 2.395191e-01\n",
      "train#13500, train loss: 8.731149e-01\n",
      "train#13510, train loss: 2.713430e-01\n",
      "train#13520, train loss: 5.931277e-01\n",
      "train#13530, train loss: 4.897374e-01\n",
      "train#13540, train loss: 1.268291e+00\n",
      "train#13550, train loss: 7.738819e-01\n",
      "train#13560, train loss: 7.659131e-01\n",
      "train#13570, train loss: 9.017258e-01\n",
      "train#13580, train loss: 7.964627e-01\n",
      "train#13590, train loss: 6.044077e-01\n",
      "train#13600, train loss: 5.009733e-01\n",
      "train#13610, train loss: 1.186551e+00\n",
      "train#13620, train loss: 8.464104e-01\n",
      "train#13630, train loss: 1.228115e+00\n",
      "train#13640, train loss: 8.491710e-01\n",
      "train#13650, train loss: 9.752451e-01\n",
      "train#13660, train loss: 1.059863e+00\n",
      "train#13670, train loss: 6.463425e-01\n",
      "train#13680, train loss: 9.513931e-01\n",
      "train#13690, train loss: 1.410908e+00\n",
      "train#13700, train loss: 1.553649e+00\n",
      "train#13710, train loss: 7.825551e-01\n",
      "train#13720, train loss: 6.058393e-01\n",
      "train#13730, train loss: 1.209335e+00\n",
      "train#13740, train loss: 8.083644e-01\n",
      "train#13750, train loss: 6.641029e-01\n",
      "train#13760, train loss: 1.053281e+00\n",
      "train#13770, train loss: 3.688562e-01\n",
      "train#13780, train loss: 9.745949e-01\n",
      "train#13790, train loss: 1.186030e+00\n",
      "train#13800, train loss: 7.808305e-01\n",
      "train#13810, train loss: 6.670311e-01\n",
      "train#13820, train loss: 7.546219e-01\n",
      "train#13830, train loss: 5.181360e-01\n",
      "train#13840, train loss: 9.408844e-01\n",
      "train#13850, train loss: 7.798384e-01\n",
      "train#13860, train loss: 6.742868e-01\n",
      "train#13870, train loss: 8.470229e-01\n",
      "train#13880, train loss: 6.765434e-01\n",
      "train#13890, train loss: 8.214076e-01\n",
      "train#13900, train loss: 7.076680e-01\n",
      "train#13910, train loss: 8.211181e-01\n",
      "train#13920, train loss: 1.057364e+00\n",
      "train#13930, train loss: 1.033261e+00\n",
      "train#13940, train loss: 7.046643e-01\n",
      "train#13950, train loss: 2.889032e-01\n",
      "train#13960, train loss: 5.113247e-01\n",
      "train#13970, train loss: 8.815162e-01\n",
      "train#13980, train loss: 6.831650e-01\n",
      "train#13990, train loss: 1.486242e+00\n",
      "prediction: \n",
      "('outputs:', 'it}\\n\\\\item If $\\\\mathcal{F}$ is pure above $y$.\\n\\\\end{enumerate}\\n\\\\end{lemma}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{lemma}\\n\\\\label{lemma-pre-flat-dimension-n}\\nLet $\\\\mathcal{F}$ is pure above $y$.\\n\\\\end{enumerate}\\n\\\\end{lemma}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{lemma}\\n\\\\label{lemma-pre-flat-dimension-n}\\nLet $\\\\mathcal{F}$ is pure above $y$.\\n\\\\end{enumerate}\\n\\\\end{lemma}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{lemma}\\n\\\\label{lemma-pre-flat-dimension-n}\\nLet $\\\\mathcal{F}$ is pure above $y$.\\n\\\\end{enumerate}\\n\\\\end{lemma}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{lemma}\\n\\\\label{lemma-pre-flat-dimension-n}\\nLet $\\\\mathcal{F}$ is pure above $y$.\\n\\\\end{enumerate}\\n\\\\end{lemma}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{lemma}\\n\\\\label{lemma-pre-flat-dimension-n}\\nLet $\\\\mathcal{F}$ is pure above $y$.\\n\\\\end{enumerate}\\n\\\\end{lemma}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{lemma}\\n\\\\label{lemma-pre-flat-dimension-n}\\nLet $\\\\mathcal{F}$ is pure above $y$.\\n\\\\end{enumerate}\\n\\\\end{lemma}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{proof}\\n\\n\\\\begin{lemma}\\n\\\\label{lemm')\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    input_ph      = tf.placeholder(tf.float32, [None, length_of_sequences, num_of_input_nodes], name=\"input\")\n",
    "    supervisor_ph = tf.placeholder(tf.float32, [None, num_of_output_nodes], name=\"supervisor\")\n",
    "    size_of_mini_batch_ph = tf.placeholder(tf.int32, name=\"size_of_mini_batch\")\n",
    "    lstm_output_keep_prob_ph = tf.placeholder(tf.float32, name=\"lstm_output_keep_prob\")\n",
    "\n",
    "    output_op, rnn_output = inference(input_ph, size_of_mini_batch_ph, lstm_output_keep_prob_ph)\n",
    "    loss_op = loss(output_op)\n",
    "    training_op = train(loss_op)\n",
    "\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.train.SummaryWriter(\"data\", graph=sess.graph)\n",
    "        \"\"\"\n",
    "        # random seed fix\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        \"\"\"\n",
    "        epoch_start = 0\n",
    "        ckpt = tf.train.get_checkpoint_state(\"./train/\")\n",
    "        saver = tf.train.Saver()\n",
    "        if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "            print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            epoch_start = int(re.search(r\"(\\d+)$\", ckpt.model_checkpoint_path).group())\n",
    "        else:\n",
    "            print(\"Created model with fresh parameters.\")\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch in range(epoch_start, num_of_training_epochs):\n",
    "            inputs, supervisors = make_mini_batch(train_data, size_of_mini_batch, length_of_sequences)\n",
    "\n",
    "            train_dict = {\n",
    "                input_ph:      inputs,\n",
    "                supervisor_ph: supervisors,\n",
    "                size_of_mini_batch_ph: size_of_mini_batch,\n",
    "                lstm_output_keep_prob_ph: 0.5,\n",
    "            }\n",
    "            rnnout, _ = sess.run([rnn_output, training_op], feed_dict=train_dict)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                summary_str, train_loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n",
    "                summary_writer.add_summary(summary_str, epoch)\n",
    "                print(\"train#%d, train loss: %e\" % (epoch, train_loss))\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                saver.save(sess, \"train/translate.ckpt\", global_step=epoch)\n",
    "\n",
    "        inputs = make_prediction_initial(train_data, 0, length_of_initial_sequences)\n",
    "        outputs = np.array([[0 for _ in range(128)]])\n",
    "\n",
    "        print(\"prediction: \")\n",
    "        for epoch in range(num_of_prediction_epochs):\n",
    "            pred_dict = {\n",
    "                input_ph:  [inputs],\n",
    "                size_of_mini_batch_ph: 1,\n",
    "                lstm_output_keep_prob_ph: 1.0\n",
    "            }\n",
    "            output = sess.run(output_op, feed_dict=pred_dict)\n",
    "            output_onehotvec = np.eye(num_of_char)[[np.argmax(output)]]\n",
    "            inputs  = np.delete(inputs, 0, 0)\n",
    "            inputs  = np.append(inputs, output_onehotvec, 0)\n",
    "            outputs = np.append(outputs, output_onehotvec, 0)\n",
    "        \n",
    "        outputs  = np.delete(outputs, 0, 0)\n",
    "        output_ascii = np.argmax(outputs.reshape(num_of_prediction_epochs, num_of_output_nodes), axis=1)\n",
    "        print(\"outputs:\", ''.join([chr(x) for x in output_ascii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
