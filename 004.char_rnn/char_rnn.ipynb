{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the next char\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_mini_batch(train_data, size_of_mini_batch, length_of_sequences):\n",
    "    inputs  = np.empty(0)\n",
    "    outputs = np.empty(0)\n",
    "    for _ in range(size_of_mini_batch):\n",
    "        index   = random.randint(0, len(train_data) - length_of_sequences - 1)\n",
    "        inputs  = np.append(inputs, train_data[index:index + length_of_sequences])\n",
    "        outputs = np.append(outputs, train_data[index + length_of_sequences])\n",
    "    inputs  = inputs.reshape(-1, length_of_sequences, 128)\n",
    "    outputs = outputs.reshape(-1, 128)\n",
    "    return (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_prediction_initial(train_data, index, length_of_sequences):\n",
    "    return train_data[index:index + length_of_sequences], train_data[index + length_of_sequences + 1]\n",
    "#inputs, _  = make_prediction_initial(train_data, 0, length_of_initial_sequences)\n",
    "#inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path             = \"./blog.txt\"\n",
    "num_of_char                 = 128\n",
    "num_of_input_nodes          = 128\n",
    "num_of_hidden_nodes         = 512\n",
    "num_of_output_nodes         = 128\n",
    "length_of_sequences         = 30\n",
    "num_of_training_epochs      = 4000\n",
    "length_of_initial_sequences = 30\n",
    "num_of_prediction_epochs    = 100\n",
    "size_of_mini_batch          = 4\n",
    "learning_rate               = 0.001\n",
    "forget_bias                 = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_data = np.load(train_data_path)\n",
    "f = open(train_data_path)\n",
    "text = f.read()\n",
    "f.close()\n",
    "train_data = np.eye(num_of_char)[[ord(x) for x in text if ord(x) < 128]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_ph, size_of_mini_batch_ph):\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "        weight1_var = tf.Variable(tf.truncated_normal([num_of_input_nodes, num_of_hidden_nodes], stddev=0.1), name=\"weight1\")\n",
    "        weight2_var = tf.Variable(tf.truncated_normal([num_of_hidden_nodes, num_of_output_nodes], stddev=0.1), name=\"weight2\")\n",
    "        bias1_var   = tf.Variable(tf.truncated_normal([num_of_hidden_nodes], stddev=0.1), name=\"bias1\")\n",
    "        bias2_var   = tf.Variable(tf.truncated_normal([num_of_output_nodes], stddev=0.1), name=\"bias2\")\n",
    "\n",
    "        weight1_hist = tf.histogram_summary(\"layer1/weights\", weight1_var)\n",
    "        weight2_hist = tf.histogram_summary(\"layer2/weights\", weight2_var)\n",
    "        bias1_hist = tf.histogram_summary(\"layer1/biases\", bias1_var)\n",
    "        bias2_hist = tf.histogram_summary(\"layer2/biases\", bias2_var)\n",
    "        \n",
    "        # pre rnn\n",
    "        in1 = tf.transpose(input_ph, [1, 0, 2])         # (batch, sequence, data) -> (sequence, batch, data)\n",
    "        in2 = tf.reshape(in1, [-1, num_of_input_nodes]) # (sequence, batch, data) -> (sequence * batch, data)\n",
    "        in3 = tf.matmul(in2, weight1_var) + bias1_var\n",
    "        in4 = tf.split(0, length_of_sequences, in3)     # sequence * (batch, data)\n",
    "\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_of_hidden_nodes, forget_bias=forget_bias, state_is_tuple=True)\n",
    "        #cell = tf.nn.rnn_cell.MultiRNNCell([_cell] * 3, state_is_tuple=True)\n",
    "        initial_state = cell.zero_state(size_of_mini_batch_ph, tf.float32)\n",
    "        rnn_output, states_op = tf.nn.rnn(cell, in4, initial_state=initial_state, dtype=tf.float32)\n",
    "        \n",
    "        # post rnn\n",
    "        #output_op = tf.matmul(rnn_output[-1], weight2_var) + bias2_var\n",
    "        out1 = tf.matmul(rnn_output[-1], weight2_var) + bias2_var\n",
    "        output_op = out1\n",
    "        #output_argmax = tf.argmax(out1, 0)\n",
    "\n",
    "    return output_op, rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output_op):\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_op, supervisor_ph))\n",
    "        loss_op = cross_entropy\n",
    "        tf.scalar_summary(\"loss\", loss_op)\n",
    "    return loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(loss_op):\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "    return training_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train#10, train loss: 4.505056e+00\n",
      "train#20, train loss: 3.833196e+00\n",
      "train#30, train loss: 2.491777e+00\n",
      "train#40, train loss: 3.930192e+00\n",
      "train#50, train loss: 2.769340e+00\n",
      "train#60, train loss: 4.062634e+00\n",
      "train#70, train loss: 2.723094e+00\n",
      "train#80, train loss: 3.352903e+00\n",
      "train#90, train loss: 2.603949e+00\n",
      "train#100, train loss: 2.300280e+00\n",
      "train#110, train loss: 2.994359e+00\n",
      "train#120, train loss: 3.676202e+00\n",
      "train#130, train loss: 2.915412e+00\n",
      "train#140, train loss: 2.170999e+00\n",
      "train#150, train loss: 2.254114e+00\n",
      "train#160, train loss: 2.858747e+00\n",
      "train#170, train loss: 2.012419e+00\n",
      "train#180, train loss: 3.522899e+00\n",
      "train#190, train loss: 4.235343e+00\n",
      "train#200, train loss: 2.771257e+00\n",
      "train#210, train loss: 3.073084e+00\n",
      "train#220, train loss: 4.253750e+00\n",
      "train#230, train loss: 2.420766e+00\n",
      "train#240, train loss: 2.118171e+00\n",
      "train#250, train loss: 2.099281e+00\n",
      "train#260, train loss: 2.002076e+00\n",
      "train#270, train loss: 2.367694e+00\n",
      "train#280, train loss: 3.327459e+00\n",
      "train#290, train loss: 2.440598e+00\n",
      "train#300, train loss: 2.403385e+00\n",
      "train#310, train loss: 2.473146e+00\n",
      "train#320, train loss: 2.320209e+00\n",
      "train#330, train loss: 2.294277e+00\n",
      "train#340, train loss: 2.364744e+00\n",
      "train#350, train loss: 3.661899e+00\n",
      "train#360, train loss: 2.673386e+00\n",
      "train#370, train loss: 3.190264e+00\n",
      "train#380, train loss: 3.166329e+00\n",
      "train#390, train loss: 3.159886e+00\n",
      "train#400, train loss: 1.882294e+00\n",
      "train#410, train loss: 2.274452e+00\n",
      "train#420, train loss: 2.277958e+00\n",
      "train#430, train loss: 3.474409e+00\n",
      "train#440, train loss: 1.740024e+00\n",
      "train#450, train loss: 2.649670e+00\n",
      "train#460, train loss: 1.874647e+00\n",
      "train#470, train loss: 2.825800e+00\n",
      "train#480, train loss: 3.402999e+00\n",
      "train#490, train loss: 3.309129e+00\n",
      "train#500, train loss: 2.778687e+00\n",
      "train#510, train loss: 1.705729e+00\n",
      "train#520, train loss: 3.133491e+00\n",
      "train#530, train loss: 2.681315e+00\n",
      "train#540, train loss: 2.572792e+00\n",
      "train#550, train loss: 2.722982e+00\n",
      "train#560, train loss: 1.357091e+00\n",
      "train#570, train loss: 2.860035e+00\n",
      "train#580, train loss: 2.164818e+00\n",
      "train#590, train loss: 1.969253e+00\n",
      "train#600, train loss: 3.264208e+00\n",
      "train#610, train loss: 3.452313e+00\n",
      "train#620, train loss: 2.497842e+00\n",
      "train#630, train loss: 3.836235e+00\n",
      "train#640, train loss: 3.099858e+00\n",
      "train#650, train loss: 2.438902e+00\n",
      "train#660, train loss: 2.671994e+00\n",
      "train#670, train loss: 1.435248e+00\n",
      "train#680, train loss: 1.528692e+00\n",
      "train#690, train loss: 3.763725e+00\n",
      "train#700, train loss: 2.301122e+00\n",
      "train#710, train loss: 2.321830e+00\n",
      "train#720, train loss: 5.316443e-01\n",
      "train#730, train loss: 1.788187e+00\n",
      "train#740, train loss: 2.110741e+00\n",
      "train#750, train loss: 1.929869e+00\n",
      "train#760, train loss: 2.936818e+00\n",
      "train#770, train loss: 2.551013e+00\n",
      "train#780, train loss: 1.704283e+00\n",
      "train#790, train loss: 2.678876e+00\n",
      "train#800, train loss: 1.260580e+00\n",
      "train#810, train loss: 1.513586e+00\n",
      "train#820, train loss: 2.550116e+00\n",
      "train#830, train loss: 1.596722e+00\n",
      "train#840, train loss: 1.674560e+00\n",
      "train#850, train loss: 2.421824e+00\n",
      "train#860, train loss: 1.389670e+00\n",
      "train#870, train loss: 1.729420e+00\n",
      "train#880, train loss: 1.513376e+00\n",
      "train#890, train loss: 1.335238e+00\n",
      "train#900, train loss: 1.337358e+00\n",
      "train#910, train loss: 2.918179e+00\n",
      "train#920, train loss: 2.827188e+00\n",
      "train#930, train loss: 2.844169e+00\n",
      "train#940, train loss: 2.698503e+00\n",
      "train#950, train loss: 2.306100e+00\n",
      "train#960, train loss: 2.646151e+00\n",
      "train#970, train loss: 1.232423e+00\n",
      "train#980, train loss: 2.762798e+00\n",
      "train#990, train loss: 1.529729e+00\n",
      "train#1000, train loss: 1.251600e+00\n",
      "train#1010, train loss: 1.698108e+00\n",
      "train#1020, train loss: 1.700294e+00\n",
      "train#1030, train loss: 2.408246e+00\n",
      "train#1040, train loss: 2.804558e+00\n",
      "train#1050, train loss: 1.836796e+00\n",
      "train#1060, train loss: 2.501031e+00\n",
      "train#1070, train loss: 2.459786e+00\n",
      "train#1080, train loss: 2.628954e+00\n",
      "train#1090, train loss: 1.648662e+00\n",
      "train#1100, train loss: 2.086918e+00\n",
      "train#1110, train loss: 1.912808e+00\n",
      "train#1120, train loss: 2.354919e+00\n",
      "train#1130, train loss: 1.343847e+00\n",
      "train#1140, train loss: 1.381268e+00\n",
      "train#1150, train loss: 8.395143e-01\n",
      "train#1160, train loss: 1.732489e+00\n",
      "train#1170, train loss: 1.947944e+00\n",
      "train#1180, train loss: 2.486782e+00\n",
      "train#1190, train loss: 1.620778e+00\n",
      "train#1200, train loss: 2.884639e+00\n",
      "train#1210, train loss: 2.666411e+00\n",
      "train#1220, train loss: 1.563732e+00\n",
      "train#1230, train loss: 1.897496e+00\n",
      "train#1240, train loss: 1.775553e+00\n",
      "train#1250, train loss: 2.710640e+00\n",
      "train#1260, train loss: 1.699581e+00\n",
      "train#1270, train loss: 1.657925e+00\n",
      "train#1280, train loss: 1.583226e+00\n",
      "train#1290, train loss: 2.257343e+00\n",
      "train#1300, train loss: 3.834770e+00\n",
      "train#1310, train loss: 2.326166e+00\n",
      "train#1320, train loss: 1.413745e+00\n",
      "train#1330, train loss: 1.546912e+00\n",
      "train#1340, train loss: 1.669722e+00\n",
      "train#1350, train loss: 1.484928e+00\n",
      "train#1360, train loss: 2.008855e+00\n",
      "train#1370, train loss: 1.025848e+00\n",
      "train#1380, train loss: 2.379735e+00\n",
      "train#1390, train loss: 2.815664e+00\n",
      "train#1400, train loss: 2.494637e+00\n",
      "train#1410, train loss: 2.520594e+00\n",
      "train#1420, train loss: 2.546304e+00\n",
      "train#1430, train loss: 2.211766e+00\n",
      "train#1440, train loss: 1.185556e+00\n",
      "train#1450, train loss: 1.286191e+00\n",
      "train#1460, train loss: 1.929989e+00\n",
      "train#1470, train loss: 1.744467e+00\n",
      "train#1480, train loss: 1.431864e+00\n",
      "train#1490, train loss: 4.947044e-01\n",
      "train#1500, train loss: 1.110438e+00\n",
      "train#1510, train loss: 9.966814e-01\n",
      "train#1520, train loss: 1.314268e+00\n",
      "train#1530, train loss: 2.581040e+00\n",
      "train#1540, train loss: 4.969791e-01\n",
      "train#1550, train loss: 1.252437e+00\n",
      "train#1560, train loss: 2.549491e+00\n",
      "train#1570, train loss: 2.315641e+00\n",
      "train#1580, train loss: 2.630508e+00\n",
      "train#1590, train loss: 1.156650e+00\n",
      "train#1600, train loss: 2.362665e+00\n",
      "train#1610, train loss: 2.190477e+00\n",
      "train#1620, train loss: 1.378902e+00\n",
      "train#1630, train loss: 2.092801e+00\n",
      "train#1640, train loss: 2.695165e+00\n",
      "train#1650, train loss: 1.548639e+00\n",
      "train#1660, train loss: 4.388762e-01\n",
      "train#1670, train loss: 2.637555e+00\n",
      "train#1680, train loss: 4.320069e-01\n",
      "train#1690, train loss: 1.259399e+00\n",
      "train#1700, train loss: 8.147467e-01\n",
      "train#1710, train loss: 8.413992e-01\n",
      "train#1720, train loss: 9.544436e-01\n",
      "train#1730, train loss: 1.108309e+00\n",
      "train#1740, train loss: 1.910834e+00\n",
      "train#1750, train loss: 9.621694e-01\n",
      "train#1760, train loss: 1.177442e+00\n",
      "train#1770, train loss: 1.626040e+00\n",
      "train#1780, train loss: 1.457564e+00\n",
      "train#1790, train loss: 4.430731e+00\n",
      "train#1800, train loss: 4.160866e-01\n",
      "train#1810, train loss: 1.084421e+00\n",
      "train#1820, train loss: 1.696144e+00\n",
      "train#1830, train loss: 1.113423e+00\n",
      "train#1840, train loss: 1.442599e+00\n",
      "train#1850, train loss: 1.545922e+00\n",
      "train#1860, train loss: 6.319327e-01\n",
      "train#1870, train loss: 1.394524e+00\n",
      "train#1880, train loss: 1.998050e+00\n",
      "train#1890, train loss: 1.798379e+00\n",
      "train#1900, train loss: 1.936981e+00\n",
      "train#1910, train loss: 2.494181e+00\n",
      "train#1920, train loss: 1.463889e+00\n",
      "train#1930, train loss: 1.098914e+00\n",
      "train#1940, train loss: 2.165746e+00\n",
      "train#1950, train loss: 1.532266e+00\n",
      "train#1960, train loss: 2.260233e-01\n",
      "train#1970, train loss: 1.849242e+00\n",
      "train#1980, train loss: 6.059290e-01\n",
      "train#1990, train loss: 7.698897e-01\n",
      "train#2000, train loss: 2.485548e+00\n",
      "train#2010, train loss: 2.764917e+00\n",
      "train#2020, train loss: 9.059473e-01\n",
      "train#2030, train loss: 1.101447e+00\n",
      "train#2040, train loss: 6.052295e-01\n",
      "train#2050, train loss: 1.643572e+00\n",
      "train#2060, train loss: 1.905715e+00\n",
      "train#2070, train loss: 9.684693e-01\n",
      "train#2080, train loss: 9.541960e-01\n",
      "train#2090, train loss: 1.002277e+00\n",
      "train#2100, train loss: 2.345833e-01\n",
      "train#2110, train loss: 1.216199e+00\n",
      "train#2120, train loss: 1.134743e+00\n",
      "train#2130, train loss: 7.548383e-01\n",
      "train#2140, train loss: 1.662749e+00\n",
      "train#2150, train loss: 1.626651e+00\n",
      "train#2160, train loss: 1.108328e+00\n",
      "train#2170, train loss: 8.603299e-01\n",
      "train#2180, train loss: 1.475423e+00\n",
      "train#2190, train loss: 4.755762e-01\n",
      "train#2200, train loss: 5.110109e-01\n",
      "train#2210, train loss: 1.046525e+00\n",
      "train#2220, train loss: 8.685375e-01\n",
      "train#2230, train loss: 9.254174e-01\n",
      "train#2240, train loss: 1.345307e-01\n",
      "train#2250, train loss: 1.162015e+00\n",
      "train#2260, train loss: 2.732494e-01\n",
      "train#2270, train loss: 7.844488e-01\n",
      "train#2280, train loss: 3.144099e+00\n",
      "train#2290, train loss: 9.028406e-01\n",
      "train#2300, train loss: 1.291358e+00\n",
      "train#2310, train loss: 9.032562e-01\n",
      "train#2320, train loss: 9.794538e-01\n",
      "train#2330, train loss: 9.339884e-01\n",
      "train#2340, train loss: 1.047460e+00\n",
      "train#2350, train loss: 3.536361e+00\n",
      "train#2360, train loss: 9.141013e-01\n",
      "train#2370, train loss: 6.227278e-01\n",
      "train#2380, train loss: 1.403334e+00\n",
      "train#2390, train loss: 1.094018e+00\n",
      "train#2400, train loss: 6.429025e-01\n",
      "train#2410, train loss: 1.552737e+00\n",
      "train#2420, train loss: 1.908596e+00\n",
      "train#2430, train loss: 9.609544e-01\n",
      "train#2440, train loss: 9.237152e-01\n",
      "train#2450, train loss: 6.386658e-01\n",
      "train#2460, train loss: 8.455663e-01\n",
      "train#2470, train loss: 4.585277e-01\n",
      "train#2480, train loss: 1.227112e+00\n",
      "train#2490, train loss: 9.094512e-01\n",
      "train#2500, train loss: 1.040612e+00\n",
      "train#2510, train loss: 1.353571e-01\n",
      "train#2520, train loss: 1.788664e+00\n",
      "train#2530, train loss: 1.445931e-01\n",
      "train#2540, train loss: 6.994830e-01\n",
      "train#2550, train loss: 5.986167e-01\n",
      "train#2560, train loss: 1.491854e+00\n",
      "train#2570, train loss: 8.848605e-01\n",
      "train#2580, train loss: 2.132639e+00\n",
      "train#2590, train loss: 1.304656e-01\n",
      "train#2600, train loss: 9.940300e-01\n",
      "train#2610, train loss: 9.806166e-01\n",
      "train#2620, train loss: 1.288130e+00\n",
      "train#2630, train loss: 2.551831e-01\n",
      "train#2640, train loss: 6.577501e-01\n",
      "train#2650, train loss: 3.818308e-01\n",
      "train#2660, train loss: 8.347604e-01\n",
      "train#2670, train loss: 1.655110e-01\n",
      "train#2680, train loss: 1.073133e+00\n",
      "train#2690, train loss: 1.018276e+00\n",
      "train#2700, train loss: 8.335268e-01\n",
      "train#2710, train loss: 1.108279e+00\n",
      "train#2720, train loss: 8.720425e-01\n",
      "train#2730, train loss: 5.463226e-01\n",
      "train#2740, train loss: 9.043959e-01\n",
      "train#2750, train loss: 1.145884e+00\n",
      "train#2760, train loss: 1.434786e+00\n",
      "train#2770, train loss: 1.334423e-01\n",
      "train#2780, train loss: 8.563088e-01\n",
      "train#2790, train loss: 3.771361e+00\n",
      "train#2800, train loss: 6.114132e-01\n",
      "train#2810, train loss: 8.842324e-01\n",
      "train#2820, train loss: 6.660551e-01\n",
      "train#2830, train loss: 1.195028e+00\n",
      "train#2840, train loss: 1.114830e+00\n",
      "train#2850, train loss: 7.809153e-01\n",
      "train#2860, train loss: 3.001074e-01\n",
      "train#2870, train loss: 1.578834e+00\n",
      "train#2880, train loss: 1.724368e+00\n",
      "train#2890, train loss: 9.767120e-01\n",
      "train#2900, train loss: 7.473478e-01\n",
      "train#2910, train loss: 7.640858e-01\n",
      "train#2920, train loss: 1.817162e+00\n",
      "train#2930, train loss: 3.792948e-01\n",
      "train#2940, train loss: 5.434772e-01\n",
      "train#2950, train loss: 9.264516e-01\n",
      "train#2960, train loss: 3.232951e-01\n",
      "train#2970, train loss: 6.068498e-01\n",
      "train#2980, train loss: 8.957006e-01\n",
      "train#2990, train loss: 6.777474e-01\n",
      "train#3000, train loss: 7.877872e-02\n",
      "train#3010, train loss: 3.889448e-01\n",
      "train#3020, train loss: 8.098963e-01\n",
      "train#3030, train loss: 8.239652e-01\n",
      "train#3040, train loss: 1.605152e+00\n",
      "train#3050, train loss: 1.103958e+00\n",
      "train#3060, train loss: 2.256703e-01\n",
      "train#3070, train loss: 9.910871e-01\n",
      "train#3080, train loss: 2.027436e+00\n",
      "train#3090, train loss: 9.635121e-01\n",
      "train#3100, train loss: 2.213173e+00\n",
      "train#3110, train loss: 5.357122e-01\n",
      "train#3120, train loss: 9.750353e-01\n",
      "train#3130, train loss: 3.129540e-01\n",
      "train#3140, train loss: 2.759371e-01\n",
      "train#3150, train loss: 9.050838e-01\n",
      "train#3160, train loss: 2.214673e+00\n",
      "train#3170, train loss: 8.457471e-01\n",
      "train#3180, train loss: 4.771811e-01\n",
      "train#3190, train loss: 1.266292e+00\n",
      "train#3200, train loss: 4.643412e-01\n",
      "train#3210, train loss: 5.935332e-01\n",
      "train#3220, train loss: 8.693074e-02\n",
      "train#3230, train loss: 6.263627e-01\n",
      "train#3240, train loss: 1.653241e+00\n",
      "train#3250, train loss: 9.048277e-01\n",
      "train#3260, train loss: 8.083504e-01\n",
      "train#3270, train loss: 9.866438e-01\n",
      "train#3280, train loss: 1.072421e+00\n",
      "train#3290, train loss: 3.144277e-01\n",
      "train#3300, train loss: 1.669979e+00\n",
      "train#3310, train loss: 3.870195e-01\n",
      "train#3320, train loss: 1.362094e+00\n",
      "train#3330, train loss: 2.785274e-01\n",
      "train#3340, train loss: 9.766192e-01\n",
      "train#3350, train loss: 5.290477e-01\n",
      "train#3360, train loss: 1.295485e+00\n",
      "train#3370, train loss: 1.141275e+00\n",
      "train#3380, train loss: 9.174963e-01\n",
      "train#3390, train loss: 2.373769e-01\n",
      "train#3400, train loss: 5.338705e-01\n",
      "train#3410, train loss: 2.030267e-01\n",
      "train#3420, train loss: 1.526686e+00\n",
      "train#3430, train loss: 2.537366e-01\n",
      "train#3440, train loss: 2.745453e-01\n",
      "train#3450, train loss: 1.277702e+00\n",
      "train#3460, train loss: 1.043181e-01\n",
      "train#3470, train loss: 1.596366e-01\n",
      "train#3480, train loss: 6.936290e-01\n",
      "train#3490, train loss: 1.361077e+00\n",
      "train#3500, train loss: 3.383639e-01\n",
      "train#3510, train loss: 1.024312e+00\n",
      "train#3520, train loss: 1.118501e-01\n",
      "train#3530, train loss: 1.102275e+00\n",
      "train#3540, train loss: 1.144712e+00\n",
      "train#3550, train loss: 1.087660e+00\n",
      "train#3560, train loss: 1.042628e+00\n",
      "train#3570, train loss: 4.198743e-01\n",
      "train#3580, train loss: 3.141550e-02\n",
      "train#3590, train loss: 3.063491e+00\n",
      "train#3600, train loss: 1.332324e-01\n",
      "train#3610, train loss: 1.155701e+00\n",
      "train#3620, train loss: 4.305781e-01\n",
      "train#3630, train loss: 2.297163e-01\n",
      "train#3640, train loss: 7.931539e-01\n",
      "train#3650, train loss: 5.687649e-02\n",
      "train#3660, train loss: 3.568065e-01\n",
      "train#3670, train loss: 4.356523e-01\n",
      "train#3680, train loss: 1.286187e+00\n",
      "train#3690, train loss: 5.440325e-01\n",
      "train#3700, train loss: 3.959296e-01\n",
      "train#3710, train loss: 1.767083e+00\n",
      "train#3720, train loss: 2.347943e-01\n",
      "train#3730, train loss: 3.582141e-01\n",
      "train#3740, train loss: 7.080022e-01\n",
      "train#3750, train loss: 1.904341e-01\n",
      "train#3760, train loss: 1.075033e+00\n",
      "train#3770, train loss: 1.111408e+00\n",
      "train#3780, train loss: 1.625195e+00\n",
      "train#3790, train loss: 4.735462e-01\n",
      "train#3800, train loss: 3.965026e-01\n",
      "train#3810, train loss: 8.935469e-02\n",
      "train#3820, train loss: 1.043340e+00\n",
      "train#3830, train loss: 1.687007e+00\n",
      "train#3840, train loss: 1.282120e+00\n",
      "train#3850, train loss: 1.108628e+00\n",
      "train#3860, train loss: 1.724862e-01\n",
      "train#3870, train loss: 3.362041e-01\n",
      "train#3880, train loss: 1.688461e-01\n",
      "train#3890, train loss: 1.899101e-01\n",
      "train#3900, train loss: 1.532035e-01\n",
      "train#3910, train loss: 1.198925e+00\n",
      "train#3920, train loss: 5.744149e-01\n",
      "train#3930, train loss: 1.062402e+00\n",
      "train#3940, train loss: 1.155393e+00\n",
      "train#3950, train loss: 3.972377e-02\n",
      "train#3960, train loss: 4.942175e-01\n",
      "train#3970, train loss: 2.545274e+00\n",
      "train#3980, train loss: 3.295288e-01\n",
      "train#3990, train loss: 1.291965e-01\n",
      "train#4000, train loss: 1.036541e-01\n",
      "prediction: \n",
      "('asciis: ', array([ 97, 108,  97, 105, 110, 116,  97, 105, 110, 105, 110, 103,  32,\n",
      "       116, 104, 101,  32, 101, 120, 116,  52,  32, 100, 114, 105, 118,\n",
      "       101, 114,  32,  98,  97, 115, 101, 100,  32, 101, 110,  99, 114,\n",
      "       121, 112, 116, 105, 111, 110,  46,  32,  70,  66,  69,  32,  97,\n",
      "       100, 100, 105, 116, 105, 111, 110,  97, 108, 108, 121,  32, 111,\n",
      "       102,  32, 116, 104, 101,  32, 101, 120, 116,  52,  32, 100, 114,\n",
      "       105, 118, 101, 114,  32,  98, 111, 111, 116,  32, 112,  97, 115,\n",
      "       115, 105, 110, 103,  32, 116, 104, 101,  32]))\n",
      "('outputs:', ['a', 'l', 'a', 'i', 'n', 't', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'e', 'x', 't', '4', ' ', 'd', 'r', 'i', 'v', 'e', 'r', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'e', 'n', 'c', 'r', 'y', 'p', 't', 'i', 'o', 'n', '.', ' ', 'F', 'B', 'E', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'e', 'x', 't', '4', ' ', 'd', 'r', 'i', 'v', 'e', 'r', ' ', 'b', 'o', 'o', 't', ' ', 'p', 'a', 's', 's', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' '])\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    input_ph      = tf.placeholder(tf.float32, [None, length_of_sequences, num_of_input_nodes], name=\"input\")\n",
    "    supervisor_ph = tf.placeholder(tf.float32, [None, num_of_output_nodes], name=\"supervisor\")\n",
    "    size_of_mini_batch_ph = tf.placeholder(tf.int32, name=\"size_of_mini_batch\")\n",
    "\n",
    "    output_op, rnn_output = inference(input_ph, size_of_mini_batch_ph)\n",
    "    loss_op = loss(output_op)\n",
    "    training_op = train(loss_op)\n",
    "\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    init = tf.initialize_all_variables()\n",
    "    #istate_ph = cell.zero_state(size_of_mini_batch, tf.float32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        summary_writer = tf.train.SummaryWriter(\"data\", graph=sess.graph)\n",
    "        # random seed fix\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        # init variables\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(num_of_training_epochs):\n",
    "            inputs, supervisors = make_mini_batch(train_data, size_of_mini_batch, length_of_sequences)\n",
    "            #state = sess.run(istate_ph)\n",
    "\n",
    "            train_dict = {\n",
    "                input_ph:      inputs,\n",
    "                supervisor_ph: supervisors,\n",
    "                size_of_mini_batch_ph: size_of_mini_batch,\n",
    "            }\n",
    "            rnnout, _ = sess.run([rnn_output, training_op], feed_dict=train_dict)\n",
    "#            print(\"rnnout: {}\".format(rnnout[-1]))\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                summary_str, train_loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n",
    "                summary_writer.add_summary(summary_str, epoch)\n",
    "                print(\"train#%d, train loss: %e\" % (epoch + 1, train_loss))\n",
    "\n",
    "        inputs, _  = make_prediction_initial(train_data, 0, length_of_initial_sequences)\n",
    "#        outputs = np.empty(0)\n",
    "        outputs = np.array([[0 for _ in range(128)]])\n",
    "\n",
    "#        istate_ph = cell.zero_state(1, tf.float32)\n",
    "#        sess.run(istate_ph)\n",
    "\n",
    "        print(\"prediction: \")\n",
    "        for epoch in range(num_of_prediction_epochs):\n",
    "            pred_dict = {\n",
    "                input_ph:  [inputs],\n",
    "                size_of_mini_batch_ph: 1\n",
    "            }\n",
    "#            output, states = sess.run([output_op, states_op], feed_dict=pred_dict)\n",
    "            output = sess.run(output_op, feed_dict=pred_dict)\n",
    "#            print(output)\n",
    "            output_onehotvec = np.eye(num_of_char)[[np.argmax(output)]]\n",
    "            inputs  = np.delete(inputs, 0, 0)\n",
    "            #inputs  = np.append(inputs, output_onehotvec, 0)\n",
    "            inputs  = np.append(inputs, output_onehotvec, 0)\n",
    "            outputs = np.append(outputs, output_onehotvec, 0)\n",
    "        \n",
    "        outputs  = np.delete(outputs, 0, 0)\n",
    "        output_ascii = np.argmax(outputs.reshape(num_of_prediction_epochs, num_of_output_nodes), axis=1)\n",
    "        print(\"asciis: \", output_ascii)\n",
    "        print(\"outputs:\", [chr(x) for x in output_ascii])\n",
    "        #saver.save(sess, \"data/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (30, 128)\n",
      "outputs: (100, 128)\n",
      "output: (1, 128)\n",
      "onehotvec: (1, 128)\n"
     ]
    }
   ],
   "source": [
    "ins = inputs\n",
    "outs = outputs\n",
    "out = output\n",
    "out_ohv = output_onehotvec\n",
    "\n",
    "print(\"inputs: {}\".format(ins.shape))\n",
    "print(\"outputs: {}\".format(outs.shape))\n",
    "print(\"output: {}\".format(out.shape))\n",
    "print(\"onehotvec: {}\".format(out_ohv.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
