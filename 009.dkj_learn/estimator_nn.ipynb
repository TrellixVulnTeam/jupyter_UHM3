{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import notebookutil as nbu\n",
    "sys.meta_path.append(nbu.NotebookFinder())\n",
    "import datasets\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import roc_auc\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fit(sess, _train_data, param, graph=False):\n",
    "    num_of_sequence_length = len(_train_data[0])\n",
    "    num_of_hidden_nodes = param['num_of_hidden_nodes']\n",
    "    num_of_training_epochs = param['num_of_training_epochs']\n",
    "    size_of_mini_batch = param['size_of_mini_batch']\n",
    "    size_of_test_batch = param['size_of_test_batch']\n",
    "    learning_rate = param['learning_rate']\n",
    "    \n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    input_ph      = tf.placeholder(tf.float32, [None, num_of_sequence_length], name=\"input\")\n",
    "    supervisor_ph = tf.placeholder(tf.float32, [None, num_of_sequence_length], name=\"supervisor\")\n",
    "    batch_size_ph = tf.placeholder(tf.int32, name=\"batch_size\")\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "        weight_var = tf.Variable(tf.truncated_normal([num_of_sequence_length, num_of_hidden_nodes], stddev=0.1), name=\"weight\")\n",
    "        bias1_var   = tf.Variable(tf.truncated_normal([num_of_hidden_nodes], stddev=0.1), name=\"bias1\")\n",
    "        bias2_var   = tf.Variable(tf.truncated_normal([num_of_sequence_length], stddev=0.1), name=\"bias2\")\n",
    "        hidden = tf.sigmoid(tf.matmul(input_ph, weight_var) + bias1_var)\n",
    "        output_op = tf.sigmoid(tf.matmul(hidden, tf.transpose(weight_var) + bias2_var))\n",
    "\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        square_error = tf.reduce_mean(tf.square(output_op - supervisor_ph))\n",
    "        loss_op      = square_error\n",
    "\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    sess.run(init)\n",
    "    accuracy_results = []\n",
    "\n",
    "    for epoch in range(num_of_training_epochs):\n",
    "        train_dict = {\n",
    "            input_ph: _train_data,\n",
    "            supervisor_ph: _train_data,\n",
    "            batch_size_ph: size_of_mini_batch,\n",
    "        }\n",
    "        sess.run(training_op, feed_dict=train_dict)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            train_loss = sess.run(loss_op, feed_dict=train_dict)\n",
    "\n",
    "            # eval accuracy\n",
    "            pred_dict = {\n",
    "                input_ph: _train_data,\n",
    "                supervisor_ph: _train_data,\n",
    "                batch_size_ph: size_of_test_batch,\n",
    "            }\n",
    "            accuracy_results.append([epoch, train_loss])\n",
    "            preddata = sess.run(output_op, feed_dict=pred_dict)\n",
    "            #print(\"train#%d, train loss: %e\" % (epoch + 1, train_loss))\n",
    "    \n",
    "    if graph == True:\n",
    "        df = pd.DataFrame({\n",
    "            'epoch': [x[0] for x in accuracy_results],\n",
    "            'train_loss': [x[1] for x in accuracy_results]\n",
    "        })\n",
    "        df.plot(x='epoch', y='train_loss')\n",
    "        df.head()\n",
    "    \n",
    "    return (input_ph, supervisor_ph, batch_size_ph), loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decision_function(sess, data, ph, loss):\n",
    "    num_of_sequence_length = len(data[0])\n",
    "    \n",
    "    input_ph,  supervisor_ph, batch_size_ph = ph\n",
    "    loss_op = loss\n",
    "    \n",
    "    _train_data = data\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for epoch in range(len(_train_data)):\n",
    "        data = _train_data[epoch].reshape(-1, num_of_sequence_length)\n",
    "\n",
    "        train_dict = {\n",
    "            input_ph: data,\n",
    "            supervisor_ph: data,\n",
    "            batch_size_ph: 1,\n",
    "        }\n",
    "\n",
    "        train_loss = sess.run(loss_op, feed_dict=train_dict)\n",
    "        scores.append(train_loss)\n",
    "        #print('test w/ train #.%d %f' % (epoch, train_loss))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class EstimatorNN(BaseEstimator):\n",
    "    def __init__(self, num_of_hidden_nodes=20, num_of_training_epochs=100, size_of_mini_batch=10,\n",
    "                 size_of_test_batch=10, learning_rate=0.02):\n",
    "        self.param = {}\n",
    "        self.param['num_of_hidden_nodes'] = num_of_hidden_nodes\n",
    "        self.param['num_of_training_epochs'] = num_of_training_epochs\n",
    "        self.param['size_of_mini_batch'] = size_of_mini_batch\n",
    "        self.param['size_of_test_batch'] = size_of_test_batch\n",
    "        self.param['learning_rate'] = learning_rate\n",
    "        self.g = tf.Graph()\n",
    "        self.g.as_default()\n",
    "        self.sess = tf.Session()\n",
    "    \n",
    "    def fit(self, x, graph=False):\n",
    "        self.ph, self.loss = fit(self.sess, x, self.param, graph=graph)\n",
    "        return self \n",
    "\n",
    "    def predict(self, x):\n",
    "        return [1.0]*len(x) \n",
    "    \n",
    "    def decision_function(self, x, y=None):\n",
    "        data = decision_function(self.sess, x, self.ph, self.loss)\n",
    "        return data\n",
    "\n",
    "    def score(self, x, y=None):\n",
    "        scores = self.decision_function(x)\n",
    "        return sum(scores)/len(scores)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return self.param\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self,parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def get_type(self):\n",
    "        return 'NN'\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'sess'):\n",
    "            self.sess.close()\n",
    "            del self.sess\n",
    "            del self.g\n",
    "            tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#''' pred area judgement from rssi\n",
    "def test():\n",
    "    train_data = np.array([[-50., -50., -50., -51., -49., -47., -45., -44., -45., -49.],\n",
    "                           [-53., -53., -53., -53., -51., -50., -50., -51., -52., -52.],\n",
    "                           [-54., -54., -54., -53., -52., -50., -48., -48., -47., -46.],\n",
    "                           [-49., -47., -46., -45., -44., -44., -42., -41., -41., -41.]])\n",
    "    test_data = np.array([[-63., -63., -64., -64., -63., -63., -63., -63., -63., -62.],\n",
    "                          [-58., -59., -59., -59., -61., -61., -59., -56., -55., -54.],\n",
    "                          [-65., -62., -62., -62., -61., -61., -60., -59., -60., -60.],\n",
    "                          [-61., -61., -62., -62., -62., -61., -60., -61., -60., -60.]])\n",
    "    print(train_data)\n",
    "    print(test_data)\n",
    "    train_data *= -0.01\n",
    "    test_data *= -0.01\n",
    "\n",
    "    nn = EstimatorNN()\n",
    "    nn.fit(train_data)\n",
    "    print(nn.decision_function(train_data))\n",
    "    print(nn.decision_function(test_data))\n",
    "    del nn\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ds = datasets.load('data/raw/02_1[1234]_0*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def eval01(title_t, title_n, title_o, sensor_type, n_record=0, n_record_after=0, drop_interval=1,\n",
    "          ma_window=1, n_train=5, normalize=True, ms_interval=20):\n",
    "    \n",
    "    drop_interval = int(ms_interval / 20)\n",
    "\n",
    "    # get data\n",
    "    dfl_b = datasets.get_data(ds, title=title_t, before=n_record, after=n_record_after,\n",
    "                              column=sensor_type, drop_interval=drop_interval)[:(n_train + 1)]\n",
    "    dfl_n = datasets.get_data(ds, title=title_n, before=n_record, after=n_record_after,\n",
    "                              column=sensor_type, drop_interval=drop_interval)\n",
    "    dfl_o = datasets.get_data(ds, title=title_o, before=n_record, after=n_record_after,\n",
    "                              column=sensor_type, drop_interval=drop_interval)\n",
    "    \n",
    "    #dfl_o = dfl_o[:1]\n",
    "    \n",
    "    #print(dfl_o)\n",
    "    #print(dfl_n)\n",
    "\n",
    "    # moving average\n",
    "    dfl_b = datasets.moving_average(dfl_b, window=ma_window, min_periods=ma_window)\n",
    "    dfl_n = datasets.moving_average(dfl_n, window=ma_window, min_periods=ma_window)\n",
    "    dfl_o = datasets.moving_average(dfl_o, window=ma_window, min_periods=ma_window)\n",
    "    \n",
    "    # normalize\n",
    "    if normalize == True:\n",
    "        dfl_b_n = datasets.normalize_by_base_data(dfl_b, dfl_b, sensor_type)\n",
    "        dfl_n_n = datasets.normalize_by_base_data(dfl_b, dfl_n, sensor_type)\n",
    "        dfl_o_n = datasets.normalize_by_base_data(dfl_b, dfl_o, sensor_type)\n",
    "\n",
    "    # get numpy array\n",
    "    data_2d_b = [df.as_matrix() for df in dfl_b_n]\n",
    "    data_2d_n = [df.as_matrix() for df in dfl_n_n]\n",
    "    data_2d_o = [df.as_matrix() for df in dfl_o_n]\n",
    "\n",
    "    # to list of numpy.array\n",
    "    data_b = [d.ravel() for d in data_2d_b]\n",
    "    data_n = [d.ravel() for d in data_2d_n]\n",
    "    data_o = [d.ravel() for d in data_2d_o]\n",
    "\n",
    "    model = EstimatorNN(num_of_hidden_nodes=8, num_of_training_epochs=400, size_of_mini_batch=10)\n",
    "    model.fit(data_b, graph=True)\n",
    "    score_n = model.decision_function(data_n)\n",
    "    score_o = model.decision_function(data_o)\n",
    "    \n",
    "    print('--normal score')\n",
    "    print(score_n)\n",
    "    print('--anomaly score')\n",
    "    print(score_o)\n",
    "    \n",
    "    auc = roc_auc.get_auc_from_normal_outlier(score_n, score_o, graph=True)\n",
    "\n",
    "    print('auc %.05f' % (auc))\n",
    "\n",
    "title_t = '02_11_01'\n",
    "title_n = '02_11'\n",
    "title_o = '02_14'\n",
    "sensor  = ['rssi.a','rssi.b']\n",
    "n_record_after = 5\n",
    "ms_interval = 100\n",
    "\n",
    "#eval01(title_t, title_n, title_o, sensor, n_record_after=n_record_after, ms_interval=ms_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test for takatani acceleration\n",
    "\n",
    "def load_dir(dir):\n",
    "    files = sorted(glob.glob(dir + '/*'))\n",
    "    dfl = []\n",
    "    for f in files:\n",
    "        #print(f)\n",
    "        df = pd.read_csv(f)\n",
    "        dfl.append(df[1:][::5][:n_record_after+1])\n",
    "\n",
    "    return dfl\n",
    "\n",
    "def eval01(title_t, title_n, title_o, sensor_type, n_record=0, n_record_after=0, drop_interval=1,\n",
    "          ma_window=1, n_train=5, normalize=True, ms_interval=20):\n",
    "    \n",
    "    drop_interval = int(ms_interval / 20)\n",
    "\n",
    "    # get data \n",
    "    dfl_b = load_dir(title_t)\n",
    "    dfl_n = load_dir(title_n)\n",
    "    dfl_o = load_dir(title_o)\n",
    "\n",
    "    # get data \n",
    "    #dfl_b = load_dir('data/takatani.acc/nexus5x_koshi_y/')\n",
    "    #dfl_n = load_dir('data/takatani.acc/nexus5x_ouka_kaban/')\n",
    "    #dfl_o = load_dir('data/takatani.acc/nexus5x_hishoji_x/')\n",
    "\n",
    "    # moving average\n",
    "    dfl_b = datasets.moving_average(dfl_b, window=ma_window, min_periods=ma_window)\n",
    "    dfl_n = datasets.moving_average(dfl_n, window=ma_window, min_periods=ma_window)\n",
    "    dfl_o = datasets.moving_average(dfl_o, window=ma_window, min_periods=ma_window)\n",
    "    dfl_b = dfl_b + dfl_n\n",
    "        \n",
    "    # normalize\n",
    "    if normalize == True:\n",
    "        dfl_b_n = datasets.normalize_by_base_data(dfl_b, dfl_b, sensor_type)\n",
    "        dfl_n_n = datasets.normalize_by_base_data(dfl_b, dfl_n, sensor_type)\n",
    "        dfl_o_n = datasets.normalize_by_base_data(dfl_b, dfl_o, sensor_type)\n",
    "\n",
    "    # get numpy array\n",
    "    data_2d_b = [df.as_matrix() for df in dfl_b_n]\n",
    "    data_2d_n = [df.as_matrix() for df in dfl_n_n]\n",
    "    data_2d_o = [df.as_matrix() for df in dfl_o_n]\n",
    "\n",
    "    # to list of numpy.array\n",
    "    data_b = [d.ravel() for d in data_2d_b]\n",
    "    data_n = [d.ravel() for d in data_2d_n]\n",
    "    data_o = [d.ravel() for d in data_2d_o]\n",
    "\n",
    "    model = EstimatorNN(num_of_hidden_nodes=8, num_of_training_epochs=400, size_of_mini_batch=10)\n",
    "    model.fit(data_b, graph=False)\n",
    "    score_n = model.decision_function(data_n)\n",
    "    score_o = model.decision_function(data_o)\n",
    "    \n",
    "    print('--normal score')\n",
    "    print(score_n)\n",
    "    print('--anomaly score')\n",
    "    print(score_o)\n",
    "    \n",
    "    auc = roc_auc.get_auc_from_normal_outlier(score_n, score_o, graph=True)\n",
    "\n",
    "    print('auc %.05f' % (auc))\n",
    "\n",
    "# galaxy_ouka_kaban'\n",
    "# galaxy_koshi_y'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "title_t = 'data/takatani.acc/nexus5xOS7_temochi/'\n",
    "title_n = 'data/takatani.acc/nexus5x_ouka_kaban/'\n",
    "title_o = 'data/takatani.acc/nexus5x_hishoji_x/'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "title_t = 'data/takatani.acc/galaxy_temochi/'\n",
    "title_n = 'data/takatani.acc/galaxy_koshi_y/'\n",
    "title_o = 'data/takatani.acc/galaxy_hishoji_z/'\n",
    "\n",
    "\n",
    "#sensor  = ['rssi.a','rssi.b']\n",
    "sensor = ['accelerometer[0]', 'accelerometer[1]', 'accelerometer[2]']\n",
    "n_record_after = 5\n",
    "ms_interval = 100\n",
    "\n",
    "#eval01(title_t, title_n, title_o, sensor, n_record_after=n_record_after, ms_interval=ms_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
