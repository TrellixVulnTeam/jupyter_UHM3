{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import notebookutil as nbu\n",
    "sys.meta_path.append(nbu.NotebookFinder())\n",
    "import datasets\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import roc_auc\n",
    "import estimator_kmeans as kmeans\n",
    "import estimator_knn as knn\n",
    "import estimator_nn as nn\n",
    "import estimator_lof as lof\n",
    "import estimator_rssibased as rssie\n",
    "from datetime import datetime\n",
    "import json\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path = data/raw/0[12]_[01][123]_0[1234]*_*\n",
      "data/raw/01_01_01_4F実験室_XperiaZ3_胸ポケット_裏上_正常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/01_01_02_4F実験室_XperiaZ3_カバン_裏上_正常_まっすぐ帰宅\n",
      "...x..........................\n",
      "data/raw/01_01_03_4F実験室_XperiaZ3_胸ポケット_裏上_正常_5秒後まっすぐ帰宅\n",
      ".........................x....\n",
      "data/raw/01_01_04_4F実験室_XperiaZ3_カバン_裏上_正常_5秒後まっすぐ帰宅\n",
      ".....................x........\n",
      "data/raw/01_02_01_4F実験室_XperiaZ3_胸ポケット_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/01_02_02_4F実験室_XperiaZ3_ズボン_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/01_02_03_4F実験室_XperiaZ3_カバン_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/01_03_01_4F実験室_XperiaZ3_胸ポケット_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/01_03_02_4F実験室_XperiaZ3_ズボン_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/01_03_03_4F実験室_XperiaZ3_カバン_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/01_11_01_エネマネハウス_XperiaZ3_胸ポケット_裏上_正常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/01_11_02_エネマネハウス_XperiaZ3_カバン_裏上_正常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/01_11_03_エネマネハウス_XperiaZ3_胸ポケット_裏上_正常_5秒後まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/01_11_04_エネマネハウス_XperiaZ3_カバン_裏上_正常_5秒後まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/01_12_01_エネマネハウス_XperiaZ3_胸ポケット_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/01_12_02_エネマネハウス_XperiaZ3_ズボン_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/01_12_03_エネマネハウス_XperiaZ3_カバン_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/01_13_01_エネマネハウス_XperiaZ3_胸ポケット_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/01_13_02_エネマネハウス_XperiaZ3_ズボン_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/01_13_03_エネマネハウス_XperiaZ3_カバン_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_01_01_4F実験室_iphone_胸ポケット_裏上_正常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_01_02_4F実験室_iphone_カバン_裏上_正常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_01_03_4F実験室_iphone_胸ポケット_裏上_正常_5秒後まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_01_04_4F実験室_iphone_カバン_裏上_正常_5秒後まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_02_01_4F実験室_iphone_胸ポケット_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/02_02_02_4F実験室_iphone_ズボン_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/02_02_03_4F実験室_iphone_カバン_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/02_03_01_4F実験室_iphone_胸ポケット_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_03_02_4F実験室_iphone_ズボン_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_03_03_4F実験室_iphone_カバン_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_11_01_エネマネハウス_iphone_胸ポケット_裏上_正常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_11_02_エネマネハウス_iphone_カバン_裏上_正常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_11_03_エネマネハウス_iphone_胸ポケット_裏上_正常_5秒後まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_11_04_エネマネハウス_iphone_カバン_裏上_正常_5秒後まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_12_01_エネマネハウス_iphone_胸ポケット_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/02_12_02_エネマネハウス_iphone_ズボン_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/02_12_03_エネマネハウス_iphone_カバン_裏上_異常_まっすぐ外出\n",
      "..............................\n",
      "data/raw/02_13_01_エネマネハウス_iphone_胸ポケット_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_13_02_エネマネハウス_iphone_ズボン_裏上_異常_まっすぐ帰宅\n",
      "..............................\n",
      "data/raw/02_13_03_エネマネハウス_iphone_カバン_裏上_異常_まっすぐ帰宅\n",
      "..............................\n"
     ]
    }
   ],
   "source": [
    "# data loader\n",
    "ds = datasets.load('data/raw/0[12]_[01][123]_0[1234]*_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# general estimator test\n",
    "def eval_estimator(\n",
    "    model,\n",
    "    sensor_type = ['rssi.a','rssi.b', ['linear_accel[0]','linear_accel[1]','linear_accel[2]']],\n",
    "    n_record = 3,\n",
    "    base = '01_11_01',\n",
    "    normal = '^01_11',\n",
    "    anomaly = '^01_1[23]',\n",
    "    ms_interval = 20,\n",
    "    ma_window = 3,\n",
    "    n_train = 3,\n",
    "    normalize = True):\n",
    "    \n",
    "    if n_record >= 0:\n",
    "        n_record_before = n_record\n",
    "        n_record_after  = 0\n",
    "    else:\n",
    "        n_record_before = 0\n",
    "        n_record_after  = -n_record\n",
    "        \n",
    "    # recalc input\n",
    "    drop_interval = int(ms_interval / 20)\n",
    "    \n",
    "    # get data\n",
    "    dfl_t = datasets.get_data(ds, title=base, before=n_record_before, after=n_record_after,\n",
    "                              column=sensor_type, drop_interval=drop_interval)[:(n_train + 1)]\n",
    "    dfl_n = datasets.get_data(ds, title=normal, before=n_record_before, after=n_record_after,\n",
    "                              column=sensor_type, drop_interval=drop_interval)\n",
    "    dfl_o = datasets.get_data(ds, title=anomaly, before=n_record_before, after=n_record_after,\n",
    "                              column=sensor_type, drop_interval=drop_interval)\n",
    "    \n",
    "    # moving average\n",
    "    dfl_t = datasets.moving_average(dfl_t, window=ma_window, min_periods=ma_window)\n",
    "    dfl_n = datasets.moving_average(dfl_n, window=ma_window, min_periods=ma_window)\n",
    "    dfl_o = datasets.moving_average(dfl_o, window=ma_window, min_periods=ma_window)\n",
    "\n",
    "    # normalize\n",
    "    if normalize == True:\n",
    "        dfl_t_n = datasets.normalize_by_base_data(dfl_t, dfl_t, sensor_type)\n",
    "        dfl_n_n = datasets.normalize_by_base_data(dfl_t, dfl_n, sensor_type)\n",
    "        dfl_o_n = datasets.normalize_by_base_data(dfl_t, dfl_o, sensor_type)\n",
    "\n",
    "    # get numpy array\n",
    "    data_2d_t = [df.as_matrix() for df in dfl_t_n]\n",
    "    data_2d_n = [df.as_matrix() for df in dfl_n_n]\n",
    "    data_2d_o = [df.as_matrix() for df in dfl_o_n]\n",
    "\n",
    "    # to list of numpy.array\n",
    "    data_t = [d.ravel() for d in data_2d_t]\n",
    "    data_n = [d.ravel() for d in data_2d_n]\n",
    "    data_o = [d.ravel() for d in data_2d_o]\n",
    "    \n",
    "    # get auc score\n",
    "    model.fit(data_t)\n",
    "        \n",
    "    score_n = model.decision_function(data_n)\n",
    "    score_o = model.decision_function(data_o)\n",
    "    auc = roc_auc.get_auc_from_normal_outlier(score_n, score_o, graph=False)\n",
    "    \n",
    "    return auc\n",
    "\n",
    "#eval_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_csv(fname):\n",
    "    df = pd.read_csv(fname, index_col=0)\n",
    "    return df\n",
    "#read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test case: 34164\n",
      "imcomplete test case: 34164\n",
      "1 / 34164 at 2017/04/29 22:20:22\n",
      "11 / 34164 at 2017/04/29 22:20:51\n",
      "21 / 34164 at 2017/04/29 22:21:21\n",
      "31 / 34164 at 2017/04/29 22:21:51\n",
      "41 / 34164 at 2017/04/29 22:22:21\n",
      "51 / 34164 at 2017/04/29 22:22:51\n",
      "61 / 34164 at 2017/04/29 22:23:22\n",
      "71 / 34164 at 2017/04/29 22:23:52\n",
      "81 / 34164 at 2017/04/29 22:24:24\n",
      "91 / 34164 at 2017/04/29 22:24:55\n",
      "101 / 34164 at 2017/04/29 22:25:40\n",
      "111 / 34164 at 2017/04/29 22:26:26\n",
      "121 / 34164 at 2017/04/29 22:27:13\n",
      "131 / 34164 at 2017/04/29 22:28:01\n",
      "141 / 34164 at 2017/04/29 22:28:51\n",
      "151 / 34164 at 2017/04/29 22:29:42\n",
      "161 / 34164 at 2017/04/29 22:30:34\n",
      "171 / 34164 at 2017/04/29 22:31:29\n",
      "181 / 34164 at 2017/04/29 22:32:25\n",
      "191 / 34164 at 2017/04/29 22:33:22\n",
      "201 / 34164 at 2017/04/29 22:34:19\n",
      "211 / 34164 at 2017/04/29 22:35:20\n",
      "221 / 34164 at 2017/04/29 22:36:21\n",
      "231 / 34164 at 2017/04/29 22:37:24\n",
      "241 / 34164 at 2017/04/29 22:38:29\n",
      "251 / 34164 at 2017/04/29 22:39:36\n",
      "261 / 34164 at 2017/04/29 22:40:46\n",
      "271 / 34164 at 2017/04/29 22:41:56\n",
      "281 / 34164 at 2017/04/29 22:43:10\n",
      "291 / 34164 at 2017/04/29 22:44:24\n",
      "301 / 34164 at 2017/04/29 22:45:42\n",
      "311 / 34164 at 2017/04/29 22:47:01\n",
      "321 / 34164 at 2017/04/29 22:48:22\n",
      "331 / 34164 at 2017/04/29 22:49:45\n",
      "341 / 34164 at 2017/04/29 22:51:09\n",
      "351 / 34164 at 2017/04/29 22:51:39\n",
      "361 / 34164 at 2017/04/29 22:51:56\n",
      "371 / 34164 at 2017/04/29 22:52:12\n",
      "381 / 34164 at 2017/04/29 22:52:30\n",
      "391 / 34164 at 2017/04/29 22:52:47\n",
      "401 / 34164 at 2017/04/29 22:53:05\n",
      "411 / 34164 at 2017/04/29 22:53:23\n",
      "421 / 34164 at 2017/04/29 22:53:41\n",
      "431 / 34164 at 2017/04/29 22:53:59\n",
      "441 / 34164 at 2017/04/29 22:54:16\n",
      "451 / 34164 at 2017/04/29 22:54:33\n",
      "461 / 34164 at 2017/04/29 22:54:50\n",
      "471 / 34164 at 2017/04/29 22:55:08\n",
      "481 / 34164 at 2017/04/29 22:55:26\n",
      "491 / 34164 at 2017/04/29 22:55:44\n",
      "501 / 34164 at 2017/04/29 22:56:02\n",
      "511 / 34164 at 2017/04/29 22:56:19\n",
      "521 / 34164 at 2017/04/29 22:56:37\n",
      "531 / 34164 at 2017/04/29 22:56:54\n",
      "541 / 34164 at 2017/04/29 22:57:11\n",
      "551 / 34164 at 2017/04/29 22:57:28\n",
      "561 / 34164 at 2017/04/29 22:57:47\n",
      "571 / 34164 at 2017/04/29 22:58:05\n",
      "581 / 34164 at 2017/04/29 22:58:23\n",
      "591 / 34164 at 2017/04/29 22:58:40\n",
      "601 / 34164 at 2017/04/29 22:58:58\n",
      "611 / 34164 at 2017/04/29 22:59:16\n",
      "621 / 34164 at 2017/04/29 22:59:34\n",
      "631 / 34164 at 2017/04/29 22:59:52\n",
      "641 / 34164 at 2017/04/29 23:00:09\n",
      "651 / 34164 at 2017/04/29 23:00:27\n",
      "661 / 34164 at 2017/04/29 23:00:45\n",
      "671 / 34164 at 2017/04/29 23:01:03\n",
      "681 / 34164 at 2017/04/29 23:01:21\n",
      "691 / 34164 at 2017/04/29 23:01:40\n",
      "701 / 34164 at 2017/04/29 23:01:59\n",
      "711 / 34164 at 2017/04/29 23:02:18\n",
      "721 / 34164 at 2017/04/29 23:02:37\n",
      "731 / 34164 at 2017/04/29 23:02:56\n",
      "741 / 34164 at 2017/04/29 23:03:16\n",
      "751 / 34164 at 2017/04/29 23:03:36\n",
      "761 / 34164 at 2017/04/29 23:03:57\n",
      "771 / 34164 at 2017/04/29 23:04:18\n",
      "781 / 34164 at 2017/04/29 23:04:38\n",
      "791 / 34164 at 2017/04/29 23:04:57\n",
      "801 / 34164 at 2017/04/29 23:05:17\n",
      "811 / 34164 at 2017/04/29 23:05:36\n",
      "821 / 34164 at 2017/04/29 23:05:56\n",
      "831 / 34164 at 2017/04/29 23:06:17\n",
      "841 / 34164 at 2017/04/29 23:06:37\n",
      "851 / 34164 at 2017/04/29 23:06:58\n",
      "861 / 34164 at 2017/04/29 23:07:17\n",
      "871 / 34164 at 2017/04/29 23:07:37\n",
      "881 / 34164 at 2017/04/29 23:07:56\n",
      "891 / 34164 at 2017/04/29 23:08:17\n",
      "901 / 34164 at 2017/04/29 23:08:37\n",
      "911 / 34164 at 2017/04/29 23:08:57\n",
      "921 / 34164 at 2017/04/29 23:09:18\n",
      "931 / 34164 at 2017/04/29 23:09:38\n",
      "941 / 34164 at 2017/04/29 23:09:58\n",
      "951 / 34164 at 2017/04/29 23:10:18\n",
      "961 / 34164 at 2017/04/29 23:10:39\n",
      "971 / 34164 at 2017/04/29 23:11:00\n",
      "981 / 34164 at 2017/04/29 23:11:20\n",
      "991 / 34164 at 2017/04/29 23:11:40\n",
      "1001 / 34164 at 2017/04/29 23:11:59\n",
      "1011 / 34164 at 2017/04/29 23:12:20\n",
      "1021 / 34164 at 2017/04/29 23:12:41\n",
      "1031 / 34164 at 2017/04/29 23:13:02\n",
      "1041 / 34164 at 2017/04/29 23:13:21\n",
      "1051 / 34164 at 2017/04/29 23:13:40\n",
      "1061 / 34164 at 2017/04/29 23:14:00\n",
      "1071 / 34164 at 2017/04/29 23:14:20\n",
      "1081 / 34164 at 2017/04/29 23:14:39\n",
      "1091 / 34164 at 2017/04/29 23:14:59\n",
      "1101 / 34164 at 2017/04/29 23:15:19\n",
      "1111 / 34164 at 2017/04/29 23:15:40\n",
      "1121 / 34164 at 2017/04/29 23:16:00\n",
      "1131 / 34164 at 2017/04/29 23:16:20\n",
      "1141 / 34164 at 2017/04/29 23:16:39\n",
      "1151 / 34164 at 2017/04/29 23:16:59\n",
      "1161 / 34164 at 2017/04/29 23:17:19\n",
      "1171 / 34164 at 2017/04/29 23:17:38\n",
      "1181 / 34164 at 2017/04/29 23:17:58\n",
      "1191 / 34164 at 2017/04/29 23:18:19\n",
      "1201 / 34164 at 2017/04/29 23:18:39\n",
      "1211 / 34164 at 2017/04/29 23:19:00\n",
      "1221 / 34164 at 2017/04/29 23:19:19\n",
      "1231 / 34164 at 2017/04/29 23:19:39\n",
      "1241 / 34164 at 2017/04/29 23:19:58\n",
      "1251 / 34164 at 2017/04/29 23:20:18\n",
      "1261 / 34164 at 2017/04/29 23:20:38\n",
      "1271 / 34164 at 2017/04/29 23:20:58\n",
      "1281 / 34164 at 2017/04/29 23:21:18\n",
      "1291 / 34164 at 2017/04/29 23:21:39\n",
      "1301 / 34164 at 2017/04/29 23:21:59\n",
      "1311 / 34164 at 2017/04/29 23:22:19\n",
      "1321 / 34164 at 2017/04/29 23:22:39\n",
      "1331 / 34164 at 2017/04/29 23:22:58\n",
      "1341 / 34164 at 2017/04/29 23:23:19\n",
      "1351 / 34164 at 2017/04/29 23:23:38\n",
      "1361 / 34164 at 2017/04/29 23:23:58\n",
      "1371 / 34164 at 2017/04/29 23:24:19\n",
      "1381 / 34164 at 2017/04/29 23:24:40\n",
      "1391 / 34164 at 2017/04/29 23:24:58\n",
      "1401 / 34164 at 2017/04/29 23:25:18\n",
      "1411 / 34164 at 2017/04/29 23:25:48\n",
      "1421 / 34164 at 2017/04/29 23:26:17\n",
      "1431 / 34164 at 2017/04/29 23:26:46\n",
      "1441 / 34164 at 2017/04/29 23:27:16\n",
      "1451 / 34164 at 2017/04/29 23:27:45\n",
      "1461 / 34164 at 2017/04/29 23:28:14\n",
      "1471 / 34164 at 2017/04/29 23:28:44\n",
      "1481 / 34164 at 2017/04/29 23:29:14\n",
      "1491 / 34164 at 2017/04/29 23:29:44\n",
      "1501 / 34164 at 2017/04/29 23:30:13\n",
      "1511 / 34164 at 2017/04/29 23:30:43\n",
      "1521 / 34164 at 2017/04/29 23:31:13\n",
      "1531 / 34164 at 2017/04/29 23:31:43\n",
      "1541 / 34164 at 2017/04/29 23:32:13\n",
      "1551 / 34164 at 2017/04/29 23:32:43\n",
      "1561 / 34164 at 2017/04/29 23:33:13\n",
      "1571 / 34164 at 2017/04/29 23:33:43\n",
      "1581 / 34164 at 2017/04/29 23:34:14\n",
      "1591 / 34164 at 2017/04/29 23:34:44\n",
      "1601 / 34164 at 2017/04/29 23:35:15\n",
      "1611 / 34164 at 2017/04/29 23:35:45\n",
      "1621 / 34164 at 2017/04/29 23:36:16\n",
      "1631 / 34164 at 2017/04/29 23:36:47\n",
      "1641 / 34164 at 2017/04/29 23:37:18\n",
      "1651 / 34164 at 2017/04/29 23:37:49\n",
      "1661 / 34164 at 2017/04/29 23:38:20\n",
      "1671 / 34164 at 2017/04/29 23:38:56\n",
      "1681 / 34164 at 2017/04/29 23:39:42\n",
      "1691 / 34164 at 2017/04/29 23:40:27\n",
      "1701 / 34164 at 2017/04/29 23:41:12\n",
      "1711 / 34164 at 2017/04/29 23:41:57\n",
      "1721 / 34164 at 2017/04/29 23:42:42\n",
      "1731 / 34164 at 2017/04/29 23:43:27\n",
      "1741 / 34164 at 2017/04/29 23:44:13\n",
      "1751 / 34164 at 2017/04/29 23:45:00\n",
      "1761 / 34164 at 2017/04/29 23:45:46\n",
      "1771 / 34164 at 2017/04/29 23:46:32\n",
      "1781 / 34164 at 2017/04/29 23:47:20\n",
      "1791 / 34164 at 2017/04/29 23:48:08\n",
      "1801 / 34164 at 2017/04/29 23:48:57\n",
      "1811 / 34164 at 2017/04/29 23:49:46\n",
      "1821 / 34164 at 2017/04/29 23:50:34\n",
      "1831 / 34164 at 2017/04/29 23:51:23\n",
      "1841 / 34164 at 2017/04/29 23:52:13\n",
      "1851 / 34164 at 2017/04/29 23:53:03\n",
      "1861 / 34164 at 2017/04/29 23:53:53\n",
      "1871 / 34164 at 2017/04/29 23:54:44\n",
      "1881 / 34164 at 2017/04/29 23:55:34\n",
      "1891 / 34164 at 2017/04/29 23:56:28\n",
      "1901 / 34164 at 2017/04/29 23:57:23\n",
      "1911 / 34164 at 2017/04/29 23:58:17\n",
      "1921 / 34164 at 2017/04/29 23:59:12\n",
      "1931 / 34164 at 2017/04/30 00:00:08\n",
      "1941 / 34164 at 2017/04/30 00:01:03\n",
      "1951 / 34164 at 2017/04/30 00:01:59\n",
      "1961 / 34164 at 2017/04/30 00:02:55\n",
      "1971 / 34164 at 2017/04/30 00:03:52\n",
      "1981 / 34164 at 2017/04/30 00:04:48\n",
      "1991 / 34164 at 2017/04/30 00:05:44\n",
      "2001 / 34164 at 2017/04/30 00:06:42\n",
      "2011 / 34164 at 2017/04/30 00:07:41\n",
      "2021 / 34164 at 2017/04/30 00:08:40\n",
      "2031 / 34164 at 2017/04/30 00:09:39\n",
      "2041 / 34164 at 2017/04/30 00:10:38\n",
      "2051 / 34164 at 2017/04/30 00:11:38\n",
      "2061 / 34164 at 2017/04/30 00:12:39\n",
      "2071 / 34164 at 2017/04/30 00:13:41\n",
      "2081 / 34164 at 2017/04/30 00:14:41\n",
      "2091 / 34164 at 2017/04/30 00:15:42\n",
      "2101 / 34164 at 2017/04/30 00:16:44\n",
      "2111 / 34164 at 2017/04/30 00:17:50\n",
      "2121 / 34164 at 2017/04/30 00:18:55\n",
      "2131 / 34164 at 2017/04/30 00:20:01\n",
      "2141 / 34164 at 2017/04/30 00:21:07\n",
      "2151 / 34164 at 2017/04/30 00:22:12\n",
      "2161 / 34164 at 2017/04/30 00:23:20\n",
      "2171 / 34164 at 2017/04/30 00:24:29\n",
      "2181 / 34164 at 2017/04/30 00:25:38\n",
      "2191 / 34164 at 2017/04/30 00:26:46\n",
      "2201 / 34164 at 2017/04/30 00:27:55\n",
      "2211 / 34164 at 2017/04/30 00:29:04\n",
      "2221 / 34164 at 2017/04/30 00:30:15\n",
      "2231 / 34164 at 2017/04/30 00:31:26\n",
      "2241 / 34164 at 2017/04/30 00:32:37\n",
      "2251 / 34164 at 2017/04/30 00:33:48\n",
      "2261 / 34164 at 2017/04/30 00:34:59\n",
      "2271 / 34164 at 2017/04/30 00:36:15\n",
      "2281 / 34164 at 2017/04/30 00:37:32\n",
      "2291 / 34164 at 2017/04/30 00:38:49\n",
      "2301 / 34164 at 2017/04/30 00:40:05\n",
      "2311 / 34164 at 2017/04/30 00:41:22\n",
      "2321 / 34164 at 2017/04/30 00:42:41\n",
      "2331 / 34164 at 2017/04/30 00:44:00\n",
      "2341 / 34164 at 2017/04/30 00:45:19\n",
      "2351 / 34164 at 2017/04/30 00:46:37\n",
      "2361 / 34164 at 2017/04/30 00:47:55\n",
      "2371 / 34164 at 2017/04/30 00:49:14\n",
      "2381 / 34164 at 2017/04/30 00:50:36\n",
      "2391 / 34164 at 2017/04/30 00:51:57\n",
      "2401 / 34164 at 2017/04/30 00:53:18\n",
      "2411 / 34164 at 2017/04/30 00:54:39\n",
      "2421 / 34164 at 2017/04/30 00:56:01\n",
      "2431 / 34164 at 2017/04/30 00:56:31\n",
      "2441 / 34164 at 2017/04/30 00:56:47\n",
      "2451 / 34164 at 2017/04/30 00:57:04\n",
      "2461 / 34164 at 2017/04/30 00:57:21\n",
      "2471 / 34164 at 2017/04/30 00:57:38\n",
      "2481 / 34164 at 2017/04/30 00:57:55\n",
      "2491 / 34164 at 2017/04/30 00:58:12\n",
      "2501 / 34164 at 2017/04/30 00:58:29\n",
      "2511 / 34164 at 2017/04/30 00:58:47\n",
      "2521 / 34164 at 2017/04/30 00:59:04\n",
      "2531 / 34164 at 2017/04/30 00:59:21\n",
      "2541 / 34164 at 2017/04/30 00:59:38\n",
      "2551 / 34164 at 2017/04/30 00:59:55\n",
      "2561 / 34164 at 2017/04/30 01:00:13\n",
      "2571 / 34164 at 2017/04/30 01:00:30\n",
      "2581 / 34164 at 2017/04/30 01:00:48\n",
      "2591 / 34164 at 2017/04/30 01:01:05\n",
      "2601 / 34164 at 2017/04/30 01:01:23\n",
      "2611 / 34164 at 2017/04/30 01:01:41\n",
      "2621 / 34164 at 2017/04/30 01:01:59\n",
      "2631 / 34164 at 2017/04/30 01:02:16\n",
      "2641 / 34164 at 2017/04/30 01:02:34\n",
      "2651 / 34164 at 2017/04/30 01:02:52\n",
      "2661 / 34164 at 2017/04/30 01:03:10\n",
      "2671 / 34164 at 2017/04/30 01:03:28\n",
      "2681 / 34164 at 2017/04/30 01:03:47\n",
      "2691 / 34164 at 2017/04/30 01:04:05\n",
      "2701 / 34164 at 2017/04/30 01:04:22\n",
      "2711 / 34164 at 2017/04/30 01:04:39\n",
      "2721 / 34164 at 2017/04/30 01:04:56\n",
      "2731 / 34164 at 2017/04/30 01:05:13\n",
      "2741 / 34164 at 2017/04/30 01:05:31\n",
      "2751 / 34164 at 2017/04/30 01:05:48\n",
      "2761 / 34164 at 2017/04/30 01:06:05\n",
      "2771 / 34164 at 2017/04/30 01:06:23\n",
      "2781 / 34164 at 2017/04/30 01:06:40\n",
      "2791 / 34164 at 2017/04/30 01:06:57\n",
      "2801 / 34164 at 2017/04/30 01:07:15\n",
      "2811 / 34164 at 2017/04/30 01:07:32\n",
      "2821 / 34164 at 2017/04/30 01:07:50\n",
      "2831 / 34164 at 2017/04/30 01:08:08\n",
      "2841 / 34164 at 2017/04/30 01:08:26\n",
      "2851 / 34164 at 2017/04/30 01:08:43\n",
      "2861 / 34164 at 2017/04/30 01:09:01\n",
      "2871 / 34164 at 2017/04/30 01:09:20\n",
      "2881 / 34164 at 2017/04/30 01:09:38\n",
      "2891 / 34164 at 2017/04/30 01:09:56\n",
      "2901 / 34164 at 2017/04/30 01:10:14\n",
      "2911 / 34164 at 2017/04/30 01:10:32\n",
      "2921 / 34164 at 2017/04/30 01:10:49\n"
     ]
    }
   ],
   "source": [
    "# grid search implementation (under test)\n",
    "def _get_estimator_models():\n",
    "    model_params = []\n",
    "    \n",
    "    # k-means based estimator\n",
    "    range_n_clusters = np.arange(1, 10, 2)\n",
    "    range_max_iter = np.array([3])\n",
    "    mesh_data = np.meshgrid(range_n_clusters, range_max_iter)\n",
    "    for n_clusters, max_iter in zip(mesh_data[0].ravel(), mesh_data[1].ravel()):\n",
    "        model_params.append({'n_clusters': n_clusters, 'max_iter': max_iter, 'type': 'k-means'})\n",
    "    \n",
    "    # kNN based estimator\n",
    "    range_n_neighbors = np.arange(1, 10, 2)\n",
    "    range_algorithm = np.array(['ball_tree'])\n",
    "    mesh_data = np.meshgrid(range_n_neighbors, range(len(range_algorithm)))\n",
    "    for n_neighbors, algorithm_idx in zip(mesh_data[0].ravel(), mesh_data[1].ravel()):\n",
    "        model_params.append({'n_neighbors': n_neighbors, 'algorithm': range_algorithm[algorithm_idx], 'type': 'kNN'})\n",
    "    \n",
    "    # LOF based estimator\n",
    "    range_n_neighbors = np.arange(1, 10, 2)\n",
    "    range_algorithm = np.array(['ball_tree'])\n",
    "    mesh_data = np.meshgrid(range_n_neighbors, range(len(range_algorithm)))\n",
    "    for n_neighbors, algorithm_idx in zip(mesh_data[0].ravel(), mesh_data[1].ravel()):\n",
    "        model_params.append({'n_neighbors': n_neighbors, 'algorithm': range_algorithm[algorithm_idx], 'type': 'LOF'})\n",
    "    \n",
    "    # NN based estimator\n",
    "    range_num_of_training_epochs = np.array([100])\n",
    "    range_num_of_hidden_nodes = np.array([4, 8, 16, 32])\n",
    "    range_size_of_mini_batch = np.array([10])\n",
    "    range_size_of_test_batch = np.array([10])\n",
    "    range_learning_rate = np.array([0.02])\n",
    "    mesh_data = np.meshgrid(range_num_of_training_epochs, range_num_of_hidden_nodes, range_size_of_mini_batch,\n",
    "                           range_size_of_test_batch, range_learning_rate)\n",
    "    for num_of_training_epochs, num_of_hidden_nodes, size_of_mini_batch, size_of_test_batch, learning_rate in zip(mesh_data[0].ravel(), mesh_data[1].ravel(), mesh_data[2].ravel(), mesh_data[3].ravel(), mesh_data[4].ravel()):\n",
    "        model_params.append({'num_of_training_epochs': num_of_training_epochs,\n",
    "                             'num_of_hidden_nodes': num_of_hidden_nodes,\n",
    "                             'size_of_mini_batch': size_of_mini_batch,\n",
    "                             'size_of_test_batch': size_of_test_batch,\n",
    "                             'learning_rate': learning_rate,\n",
    "                             'type': 'NN'\n",
    "                            })\n",
    "    \n",
    "    # rssi based estimator\n",
    "    #models.append(rssie.EstimatorRssiBased())\n",
    "    model_params.append({'type': 'rssi_based'})\n",
    "    \n",
    "    return model_params\n",
    "\n",
    "def _get_grid_test_case(model_params):\n",
    "    # parameters other than algorithm specific\n",
    "    sensor_master_ios = [\n",
    "        ['rssi.a', 'rssi.b'],\n",
    "        ['rssi.a', 'rssi.b', ['acceleration.x', 'acceleration.y', 'acceleration.z']],\n",
    "        ['rssi.a', 'rssi.b', ['gyro.rotationRate.x', 'gyro.rotationRate.y', 'gyro.rotationRate.z']],\n",
    "        ['rssi.a', 'rssi.b', ['magneticField.x', 'magneticField.y', 'magneticField.z']],\n",
    "        ['rssi.a', 'rssi.b', ['attitude.roll', 'attitude.pitch', 'attitude.yaw']],\n",
    "        ['rssi.a', 'rssi.b', ['rotationRate.x', 'rotationRate.y', 'rotationRate.z']],\n",
    "        ['rssi.a', 'rssi.b', ['gravity.x', 'gravity.y', 'gravity.z']],\n",
    "        ['rssi.a', 'rssi.b', ['userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']],\n",
    "    ]\n",
    "    sensor_master_android = [\n",
    "        ['rssi.a', 'rssi.b'],\n",
    "        ['rssi.a', 'rssi.b', ['linear_accel[0]', 'linear_accel[1]', 'linear_accel[2]']],\n",
    "        ['rssi.a', 'rssi.b', ['accelerometer[0]', 'accelerometer[1]', 'accelerometer[2]']],\n",
    "        ['rssi.a', 'rssi.b', ['gravity[0]', 'gravity[1]', 'gravity[2]']],\n",
    "        ['rssi.a', 'rssi.b', ['gyro[0]', 'gyro[1]', 'gyro[2]']],\n",
    "        ['rssi.a', 'rssi.b', ['rotation[0]', 'rotation[1]', 'rotation[2]']],\n",
    "        ['rssi.a', 'rssi.b', ['game_rotation[0]', 'game_rotation[1]', 'game_rotation[2]']],\n",
    "        ['rssi.a', 'rssi.b', ['magnetic[0]', 'magnetic[1]', 'magnetic[2]']],\n",
    "        ['rssi.a', 'rssi.b', 'pressure'],\n",
    "        #['rssi.a', 'rssi.b', 'light'],\n",
    "    ]\n",
    "    #sensor_master_s = [json.dumps(s) for s in sensor_master_ios] # to be combinationable\n",
    "    sensor_master_s = [json.dumps(s) for s in sensor_master_android] # to be combinationable\n",
    "\n",
    "    range_n_record = np.array([-20, -5, -1, 0, 1, 5, 20])\n",
    "    range_n_train = np.array([1, 5, 10, 20, 100])\n",
    "    range_ms_interval = np.array([20, 60, 100])\n",
    "    range_ma_window = np.arange(1, 4, 2)\n",
    "    \n",
    "    mesh_data = np.meshgrid(range(len(model_params)), range_n_record, range_n_train,\n",
    "                            range_ms_interval, range_ma_window, range(len(sensor_master_s)))\n",
    "\n",
    "    # create grid test case\n",
    "    test_case = []\n",
    "    for model_param_idx, n_record, n_train, ms_interval, ma_window, sensor_idx in zip(mesh_data[0].ravel(), mesh_data[1].ravel(), mesh_data[2].ravel(),\n",
    "               mesh_data[3].ravel(), mesh_data[4].ravel(), mesh_data[5].ravel()):\n",
    "        # save test case and result\n",
    "        _t = {'n_record': n_record, 'n_train': n_train,\n",
    "              'ms_interval': ms_interval, 'ma_window': ma_window, 'sensor_type': sensor_master_s[sensor_idx]}\n",
    "        _t.update(model_params[model_param_idx])\n",
    "        test_case.append(_t)\n",
    "    \n",
    "    # create df for test case\n",
    "    df = pd.DataFrame(test_case)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def _remove_unavailable_test_case(df):\n",
    "    # shold be done after the merge the existing csv\n",
    "    \n",
    "    # remove unavailable test case\n",
    "    df = df[~(df['n_train'] < df['n_neighbors'])]\n",
    "    df = df[~(df['n_train'] < df['n_clusters'])]\n",
    "    df = df[~((df['type'] == 'rssi_based') & (df['sensor_type'].apply(lambda x: len(json.loads(x)) > 2)))]\n",
    "    df = df[~((df['n_record'] >= 0) & (df['n_record'] - df['ma_window'] < 0))]\n",
    "    df = df[~((df['n_record'] < 0) & (-df['n_record'] - df['ma_window'] < 0))]\n",
    "    df = df[~((df['n_record'] < 0) & (-df['n_record'] * df['ms_interval'] > 1000))]\n",
    "    return df\n",
    "\n",
    "def _run_test(df):\n",
    "    # set the data title\n",
    "    base = '01_01_01'\n",
    "    normal = '^01_01'\n",
    "    anomaly = '^01_0[23]'\n",
    "    \n",
    "    csv_fname = 'test_record_%s__%s__%s.csv' % (base, normal, anomaly) \n",
    "\n",
    "    # if auc is already computed in some test case, merge the result. \n",
    "    if os.path.exists(csv_fname):\n",
    "        df_past = read_csv(csv_fname)\n",
    "        keys = list(df.columns.values)\n",
    "        df = pd.merge(df, df_past, on=keys, how='outer')\n",
    "    else:\n",
    "        df['auc'] = np.nan\n",
    "    \n",
    "    # show test size\n",
    "    imcomplete_test_case = len([x for x in df['auc'].values if np.isnan(x)])\n",
    "    print('total test case: %d' % (len(df.index)))\n",
    "    print('imcomplete test case: %d' %(imcomplete_test_case))\n",
    "        \n",
    "    # run test\n",
    "    for i, (k, t) in enumerate(df[df['auc'].isnull()].iterrows()):\n",
    "        if i % 10 == 0:\n",
    "            print(\"%d / %d at %s\"%(i+1, imcomplete_test_case, datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")))\n",
    "            df.to_csv(csv_fname )\n",
    "\n",
    "        # generate model from parameter\n",
    "        if t['type'] == 'k-means':\n",
    "            model = kmeans.EstimatorKmeans(n_clusters=int(t['n_clusters']), max_iter=int(t['max_iter']))\n",
    "        elif t['type'] == 'kNN':\n",
    "            model = knn.EstimatorKNN(n_neighbors=int(t['n_neighbors']), algorithm=t['algorithm'])\n",
    "        elif t['type'] == 'LOF':\n",
    "            model = lof.EstimatorLOF(n_neighbors=int(t['n_neighbors']), algorithm=t['algorithm'])\n",
    "        elif t['type'] == 'NN':\n",
    "            model = nn.EstimatorNN(num_of_hidden_nodes=int(t['num_of_hidden_nodes']),\n",
    "                                   num_of_training_epochs=int(t['num_of_training_epochs']),\n",
    "                                   size_of_mini_batch=int(t['size_of_mini_batch']),\n",
    "                                   size_of_test_batch=int(t['size_of_test_batch']),\n",
    "                                   learning_rate=t['learning_rate'])\n",
    "        elif t['type'] == 'rssi_based':\n",
    "            model = rssie.EstimatorRssiBased()\n",
    "        else:\n",
    "            print('! \"%s\" is not defined'%(t['type']))\n",
    "            return None\n",
    "        \n",
    "        #print(model.get_params())\n",
    "        \n",
    "        #  run a test\n",
    "        auc = eval_estimator(model, base=base, normal=normal, anomaly=anomaly,\n",
    "                             n_train=t['n_train'], sensor_type=json.loads(t['sensor_type']), n_record=t['n_record'],\n",
    "                             ms_interval=t['ms_interval'], ma_window=t['ma_window'])\n",
    "        df.loc[[k], 'auc'] = auc\n",
    "        \n",
    "        # deallocate memory space for the model\n",
    "        del model\n",
    "    \n",
    "    df.to_csv(csv_fname )\n",
    "    return df\n",
    "\n",
    "def test():\n",
    "    model_params = _get_estimator_models()\n",
    "    df = _get_grid_test_case(model_params)\n",
    "    df = _remove_unavailable_test_case(df)\n",
    "    df = _run_test(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
